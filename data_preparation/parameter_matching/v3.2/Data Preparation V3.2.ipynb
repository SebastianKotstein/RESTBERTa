{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7aeded-40de-476f-a82f-c6f9e205965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes (v3.1):\n",
    "# - We added and renamed several counters (metrics)\n",
    "# - Bug-Fix: In v3, the script counts all properties with too deep paths and having no descriptions, but only parameters, i.e., properties being a leaf, should be considered. \n",
    "# - We removed the verification whether the context is empty in create_question_answer_samples_for_payload\n",
    "\n",
    "# Changes (v3.2):\n",
    "# - We analyse the description length of all schema properties\n",
    "# - As well as the description length in the final QA samples and the length in the schema and the number of parameters per schema\n",
    "# - We added further counters (metrics) addressing the number of processed payload samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db764115-9c86-404c-bda7-536740ef8768",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46202d2d-73a7-43c3-872d-eecc5ddef256",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d24e41-6ef1-4fdb-8964-f65b1b8e2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ac28f-6a2a-46f7-836a-ae051ee810a6",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76bfa97-ed47-4252-81a4-0328362a00c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import uuid\n",
    "import operator\n",
    "import os\n",
    "\n",
    "# tqdm is used to visualize the progress while processing input files\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for embedding current time into log file name \n",
    "from datetime import datetime\n",
    "\n",
    "# We will use a pre-trained tokenizer to determine the length of strings\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# for description lenght analysis (added in v3.2)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146f744-113e-4727-8cf3-03c94b468160",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0df4f-fa6a-4149-95fa-ed3849cdc94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if set, only the first 'n' API tree models will be loaded\n",
    "api_limit = None\n",
    "\n",
    "remove_uris = True  # remove URIs from description\n",
    "sort_by_name = True # sort context by name\n",
    "\n",
    "max_depth = 8 # max. depth of XPath in both context and answer\n",
    "min_question_length = 3 # min. number of tokens that must be in a question\n",
    "max_question_length = 96 # max. number of tokens that may be in a question\n",
    "\n",
    "max_questions_per_sample = 32 # max. number of Question-Answer pairs per sample. If number is exceeded, additional sample is created\n",
    "\n",
    "number_of_chunks = 10 # number of containers where samples are distributed to\n",
    "\n",
    "# Variable that specifies how many times the generated sample set is repeated. A value of '1' means that each sample is only created once.\n",
    "original_retakes = 1\n",
    "shuffled_retakes = 0\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Specify the base model that is used for training later. We will use its pre-trained tokenizer in this notebook.\n",
    "base_model = \"microsoft/codebert-base\"\n",
    "\n",
    "# List of special tokens that should be removed from XPaths while creating context string\n",
    "to_be_removed = [\"<?>\",\"<str>\",\"<num>\",\"<int>\",\"<bool>\",\"{_}\",\"$.\"]\n",
    "\n",
    "input_path = \"/home/user/input_directory/\"\n",
    "output_path = \"/home/user/output_directory/\"\n",
    "\n",
    "# APIs (identified by their keys) that should be excluded from processing after loading and parsing them (e.g. due to too many payloads)\n",
    "excluded_api_keys = []\n",
    "\n",
    "# (added in v3.2)\n",
    "pre_description_length_analysis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807b27b-ad26-43d2-9a6f-7b3bb1d61c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (added in v3.1) number of processed parameters, i.e., properties that have a primitive type and, therefore, are candidates for question-answer pairs\n",
    "cnt_parameters = 0 \n",
    "# (added in v3.1) number of processed properties\n",
    "cnt_properties = 0\n",
    "# (added in v3.1) number of parameters that do not satisfy the contraints required to generate a question-answer pair\n",
    "cnt_invalid_parameters = 0 \n",
    "\n",
    "# (added in v3.2) number of parameters whose descriptions do not satisfy at least one constraint\n",
    "cnt_parameter_with_description_constraint_violation = 0\n",
    "# (renamed in v3.1) number of parameters that must be excluded due to a missing description\n",
    "cnt_parameters_without_descriptions = 0 \n",
    "# (renamed in v3.1) number of parameters whose XPath exceeds the maximum depth\n",
    "cnt_parameters_with_too_deep_xpath = 0\n",
    "# (renamed in v3.1) number of paramters whose descriptions are too short\n",
    "cnt_parameters_with_too_short_descriptions = 0 \n",
    "# (added in v3.1) number of parameters whose description cannot be truncated, since:\n",
    "# 1.) The description is still too long, even after removing trailing sentences (i.e., truncate_question returns an empty description)\n",
    "# 2.) The description could be truncated, but the resultung description does not contain enough tokens\n",
    "cnt_parameters_with_descriptions_that_could_not_be_truncated = 0 \n",
    "\n",
    "# (renamed in v3.1) parameters with descriptions that are truncated\n",
    "cnt_parameters_with_truncated_descriptions = 0 \n",
    "\n",
    "# (added in v3.2) number of payloads from which at least one question-answer pair can be generated\n",
    "cnt_payload_with_samples = 0\n",
    "# (renamed in v3.1) number of payloads from which no question-answer pairs can be generated, since the payload does not contain any parameters that satisfies the constraints\n",
    "cnt_payload_without_samples = 0 \n",
    "# (added in v3.2) number of payloads that do not contain one element (i.e., can be considered as empty)\n",
    "cnt_empty_payloads = 0\n",
    "\n",
    "# (added in v3.1) parameters that are excluded from processing since the schema (context) is empty, \n",
    "# i.e., the schema does not contain any parameter or the XPaths of all parameters of the schema exceeds the the maximum depth \n",
    "#cnt_excluded_parameters = 0\n",
    "# (added in v3.1) properties that are excluded from processing since the schema (context) is empty\n",
    "# i.e., the schema does not contain any parameter or the XPaths of all parameters of the schema exceeds the the maximum depth \n",
    "#cnt_excluded_properties = 0\n",
    "\n",
    "cnt_split_samples = 0\n",
    "cnt_answer_not_in_context = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8570ec-cf40-49a7-b65e-e1c4f6c44914",
   "metadata": {},
   "source": [
    "# Load and Parse API Tree Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a423970-e75f-43c2-9956-a9011f131f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_data_types_from_xpath(xpath: str):\n",
    "    \"\"\"\n",
    "    Removes special tokens defined in 'to_be_removed' from the passed XPath (input string) and returns the modified string\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xpath : str\n",
    "        input string\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Modified input string\n",
    "    \"\"\"\n",
    "    for data_type in to_be_removed:\n",
    "        xpath = xpath.replace(data_type,\"\")\n",
    "    return xpath\n",
    "\n",
    "\n",
    "class ApiInterfaceNode:\n",
    "    \"\"\"\n",
    "    Represents a generic node in an API tree.\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    key : str\n",
    "        key that uniquely identifies the node among all children of the parent node\n",
    "    value\n",
    "        optional value of the node\n",
    "    node_type: str\n",
    "        type (i.e. role) of the node, allowed values are 'api', 'path', 'method', 'response', 'payload', and 'property'\n",
    "    id : str\n",
    "        unique identifier of the node among all nodes of the API tree\n",
    "    elements : ApiInterfaceNode\n",
    "        contains all children of the node\n",
    "    raw_node\n",
    "        contains the original JSON structure of the node and the sub tree as Python dictionary\n",
    "    api_key : str\n",
    "        contains the API key, this attribute is only present if node is type of 'api'\n",
    "    api_name: str\n",
    "        contains the API name, this attribute is only present if node is type of 'api'\n",
    "    api_version_key : str\n",
    "        contains the API version key, this attribute is only present if node is type of 'api'\n",
    "    api_version_name : str\n",
    "        contains the API version name, this attribute is only present if node is type of 'api'\n",
    "    method_summary : str\n",
    "        contains the summary of the method, this attribute is only present if node is type of 'method'\n",
    "    method_description : str\n",
    "        contains the description of the method, this attribute is only present if node is type of 'method'\n",
    "    response_description : str\n",
    "        contains the description of the response, this attribute is only present if node is type of 'response'\n",
    "    property_name : str\n",
    "        contains the name of the property, this attribute is only present if node is type of 'property'\n",
    "    property_data_type : str\n",
    "        contains the data type of the property, this attribute is only present if node is type of 'property'\n",
    "    property_xpath : str\n",
    "        contains the XPath of the property, this attribute is only present if node is type of 'property'\n",
    "    property_format : str\n",
    "        contains the format of the property, this attribute is only present if node is type of 'property'\n",
    "    property_pattern : str\n",
    "        contains the pattern of the property, this attribute is only present if node is type of 'property'\n",
    "    property_description : str\n",
    "        contains the description of the property, this attribute is only present if node is type of 'property'\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    is_type(node_type : str):\n",
    "        Returns true if the node's type is equal the passed type\n",
    "    __str__(): \n",
    "        Returns a JSON object as string containing all attributes of the node\n",
    "    \"\"\"\n",
    "    def __init__(self, api_documentation_raw_node):\n",
    "        \"\"\"\n",
    "        Constructs the sub tree consisting of ApiInterfaceNodes based on the passed raw API tree model (parsed JSON structrure as Python dictionary).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        api_documentation_raw_node\n",
    "            parsed JSON structure of the API tree model as Python dictionary\n",
    "        \"\"\"\n",
    "        self.raw_node = api_documentation_raw_node\n",
    "\n",
    "        # Generic attributes\n",
    "        self.key = api_documentation_raw_node[\"key\"]\n",
    "        self.value = api_documentation_raw_node[\"value\"]\n",
    "        self.node_type = api_documentation_raw_node[\"type\"]\n",
    "        self.id = api_documentation_raw_node[\"id\"].replace(\"-\",\".\")\n",
    "        #self.id = parent_id+\".\"+self.key\n",
    "\n",
    "        self.elements = [ApiInterfaceNode(api_documentation_raw_node[\"elements\"][i]) for i in range(len(api_documentation_raw_node[\"elements\"]))]\n",
    "    \n",
    "        if self.node_type == \"api\":\n",
    "            self.api_key = api_documentation_raw_node[\"apiKey\"]\n",
    "            self.api_name = api_documentation_raw_node[\"apiName\"]\n",
    "            self.api_version_key = api_documentation_raw_node[\"versionKey\"]\n",
    "            self.api_version_name = api_documentation_raw_node[\"versionName\"]\n",
    "    \n",
    "        if self.node_type == \"path\":\n",
    "            # no individual attributes for path type\n",
    "            pass\n",
    "\n",
    "        if self.node_type == \"method\":\n",
    "            self.method_summary = api_documentation_raw_node[\"summary\"]\n",
    "            self.method_description = api_documentation_raw_node[\"description\"]\n",
    "\n",
    "        if self.node_type == \"response\":\n",
    "            self.response_description = api_documentation_raw_node[\"description\"]\n",
    "    \n",
    "        if self.node_type == \"payload\":\n",
    "            # no individual attributes for payload type\n",
    "            pass\n",
    "\n",
    "        if self.node_type == \"property\":\n",
    "            self.property_name = api_documentation_raw_node[\"name\"]\n",
    "            self.property_data_type = api_documentation_raw_node[\"dataType\"]\n",
    "            self.property_xpath = remove_data_types_from_xpath(api_documentation_raw_node[\"xpath\"].replace(' ','').replace('\\t','').replace('\\n',''))\n",
    "            self.property_format = api_documentation_raw_node[\"format\"]\n",
    "            self.property_pattern = api_documentation_raw_node[\"pattern\"]\n",
    "            self.property_description = api_documentation_raw_node[\"description\"]\n",
    "        \n",
    "    def is_type(self, node_type: str):\n",
    "        \"\"\"\n",
    "        Returns true, if the node's type is equal the passed type, else false.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_type : str\n",
    "            type that should be compared with the type of the node\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        True or False\n",
    "        \"\"\"\n",
    "        return self.node_type == node_type\n",
    "  \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a JSON object as string containing all attributes of the node\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        JSON object as string containing all attributes of the node\n",
    "        \"\"\"\n",
    "        json_dict = {}\n",
    "        json_dict[\"key\"] = self.key\n",
    "        json_dict[\"value\"] = self.value\n",
    "        json_dict[\"id\"] = self.id\n",
    "        json_dict[\"type\"] = self.node_type\n",
    "        json_dict[\"number_of_elements\"] = len(self.elements)\n",
    "\n",
    "        if self.node_type == \"api\":\n",
    "            json_dict[\"apiKey\"] = self.api_key\n",
    "            json_dict[\"apiName\"] = self.api_name\n",
    "            json_dict[\"versionKey\"] = self.api_version_key\n",
    "            json_dict[\"versionName\"] = self.api_version_name\n",
    "    \n",
    "        if self.node_type == \"path\":\n",
    "            # no individual attributes for path type\n",
    "            pass\n",
    "\n",
    "        if self.node_type == \"method\":\n",
    "            json_dict[\"summary\"] = self.method_summary\n",
    "            json_dict[\"description\"] = self.method_description\n",
    "\n",
    "        if self.node_type == \"response\":\n",
    "            json_dict[\"description\"] = self.response_description\n",
    "    \n",
    "        if self.node_type == \"payload\":\n",
    "            # no individual attributes for payload type\n",
    "            pass\n",
    "\n",
    "        if self.node_type == \"property\":\n",
    "            json_dict[\"name\"] = self.property_name \n",
    "            json_dict[\"dataType\"] = self.property_data_type \n",
    "            json_dict[\"xpath\"] = self.property_xpath \n",
    "            json_dict[\"format\"] = self.property_format\n",
    "            json_dict[\"pattern\"] = self.property_pattern\n",
    "            json_dict[\"description\"] = self.property_description\n",
    "    \n",
    "        return json.dumps(json_dict)\n",
    "\n",
    "def load_and_parse_api(path: str):\n",
    "    \"\"\"\n",
    "    Loads and parses an API tree model file located under the passed path and converts it structure into a structure of ApiInterfaceNodes.\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path of the API tree model file\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    ApiInterfaceNode representing the root of the loaded and parsed API tree model\n",
    "    \"\"\"\n",
    "    with open(path,\"r\",encoding=\"utf-8\") as json_file:\n",
    "        json_api = json.load(json_file)\n",
    "    return ApiInterfaceNode(json_api)\n",
    "\n",
    "def load_and_parse_apis_from_directory(path: str, limit: int = None):\n",
    "    \"\"\"\n",
    "    Loads and parses multiple API tree model files located in the specified directory.\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to directory where the API tree model files are located\n",
    "      \n",
    "    limit : int\n",
    "        Optional limit. If specified, only the first 'n' API tree model files are loaded and parsed\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    List of ApiInterfaceNodes where each node represents the root of a loaded and parsed API tree model\n",
    "    \"\"\"\n",
    "    apis = []\n",
    "    if limit:\n",
    "        filesnames = os.listdir(path)[:limit]\n",
    "    else:\n",
    "        filesnames = os.listdir(path)\n",
    "\n",
    "    for filename in tqdm(filesnames):\n",
    "        if filename.endswith(\".json\"):\n",
    "            apis.append(load_and_parse_api(os.path.join(path,filename)))\n",
    "    return apis\n",
    "\n",
    "def extract_nodes(node: ApiInterfaceNode, node_type: str):\n",
    "    \"\"\"\n",
    "    Extracts all nodes matching the passed node type from the passed API tree model.\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : ApiInterfaceNode\n",
    "      API tree model (input)\n",
    "    node_type : str\n",
    "      The type of the node that should be extracted\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of ApiInterfaceNodes matching the passed node type\n",
    "    \"\"\"\n",
    "    nodes = []\n",
    "    if node.node_type == node_type:\n",
    "        nodes.append(node)\n",
    "    for element in node.elements:\n",
    "        nodes += extract_nodes(element, node_type)\n",
    "    return nodes\n",
    "\n",
    "def extract_nodes_in_apis(nodes: [ApiInterfaceNode], node_type: str):\n",
    "    \"\"\"\n",
    "    Extracts all nodes matching the passed node type from the passed list of API tree models.\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : [ApiInterfaceNodes]\n",
    "      List of API tree models (input)\n",
    "    node_type : str\n",
    "      The type of the node that should be extracted\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of ApiInterfaceNodes matching the passed node type\n",
    "    \"\"\"\n",
    "    extracted_nodes = []\n",
    "    for node in nodes:\n",
    "        extracted_nodes += extract_nodes(node,node_type)\n",
    "    return extracted_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a630a1-9c87-4b8f-b9ef-453208c85613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse API tree models\n",
    "apis = load_and_parse_apis_from_directory(input_path,limit=api_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9431fdbf-4f9a-4139-92a7-fcb2e32b428a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Pre-Trained Tokenizer for Length Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39150aa-dac0-4b72-8cbb-4b583fc5a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03f9c0-cda7-4453-8b02-21a18f29104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(input: str):\n",
    "    \"\"\"\n",
    "    Calculates and returns the length (i.e. number of tokens) of the passed input string\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input : str\n",
    "        Input string whose length should be calculated\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Number of tokens\n",
    "    \"\"\"\n",
    "    return len(tokenizer.encode(input, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23448e1-97b6-46f0-9ed9-60d7b71b7758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tokenizer\n",
    "test_string = \"account.user.name\"\n",
    "get_length(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab87cd8-fb86-49c2-8902-60952d1c20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing length limitations\n",
    "s = \" \".join([str(x) for x in range(99999)])\n",
    "get_length(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b579ed91-0a75-4bb4-8bf6-918e3f021525",
   "metadata": {},
   "source": [
    "# Description Length Analysis (added in v3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7a01b-7f22-4db1-87c5-4721cc401d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract payload nodes\n",
    "payload_nodes = extract_nodes_in_apis(apis,node_type=\"payload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ffcbf-d593-4ecc-bd1d-faa881f2cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of payload nodes: \",len(payload_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56103b65-943a-4d88-bcd9-960d881694fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_description_length_analysis:    \n",
    "    non_parameter_descriptions = []\n",
    "    parameter_descriptions = []\n",
    "    num_non_parameters = 0\n",
    "    num_parameters =  0\n",
    "\n",
    "    def analyze_property(node: ApiInterfaceNode):\n",
    "        if (node.property_data_type == \"string\" \n",
    "            or node.property_data_type == \"number\" \n",
    "            or node.property_data_type == \"integer\" \n",
    "            or node.property_data_type == \"boolean\"\n",
    "            or (node.property_data_type == \"unknown\" and len(node.elements) == 0)):\n",
    "\n",
    "            global num_parameters\n",
    "            num_parameters+=1\n",
    "            if node.property_description:\n",
    "                parameter_descriptions.append(node.property_description)\n",
    "\n",
    "        # recursive call if node is type of array, object, or unknown with childs\n",
    "        if node.property_data_type == \"array\" or node.property_data_type == \"object\" or ((node.property_data_type == \"unknown\" or node.property_data_type == None) and len(node.elements) > 0):\n",
    "\n",
    "            global num_non_parameters\n",
    "            num_non_parameters+=1\n",
    "\n",
    "            if node.property_description:\n",
    "                non_parameter_descriptions.append(node.property_description)\n",
    "\n",
    "            for element in node.elements:\n",
    "                analyze_property(element)\n",
    "\n",
    "\n",
    "    for payload_node in payload_nodes:\n",
    "        # if payload node has a root element, i.e. \"$\"\n",
    "        if len(payload_node.elements) == 1:\n",
    "            analyze_property(payload_node.elements[0]) \n",
    "        \n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f969fff2-2f5d-4734-a2e2-a1aa45e7e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_description_length_analysis: \n",
    "    print(\"Number of objects and arrays: \",num_non_parameters)\n",
    "    print(\"Number of parameters: \",num_parameters)\n",
    "    print(\"Number of objects and arrays with description: \", len(non_parameter_descriptions))\n",
    "    print(\"Number of parameters with description: \", len(parameter_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b422765-1216-48c4-bc71-88096cc7c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_description_length_analysis: \n",
    "    non_parameter_descriptions_token_len = []\n",
    "    non_parameter_descriptions_word_len = []\n",
    "    parameter_descriptions_token_len = []\n",
    "    parameter_descriptions_word_len = []\n",
    "\n",
    "    for description in tqdm(non_parameter_descriptions):\n",
    "        non_parameter_descriptions_token_len.append(get_length(description))\n",
    "        non_parameter_descriptions_word_len.append(len(word_tokenize(description)))\n",
    "\n",
    "    for description in tqdm(parameter_descriptions):\n",
    "        parameter_descriptions_token_len.append(get_length(description))\n",
    "        parameter_descriptions_word_len.append(len(word_tokenize(description)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d58f5-9bfc-4728-9710-bfb777ab0bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_description_length_analysis: \n",
    "    print(\"Mean|median|stdev|min|max for description token length for objects and arrays: \", statistics.mean(non_parameter_descriptions_token_len),\"|\",statistics.median(non_parameter_descriptions_token_len),\"|\",statistics.stdev(non_parameter_descriptions_token_len),\"|\",min(non_parameter_descriptions_token_len),\"|\",max(non_parameter_descriptions_token_len))\n",
    "    print(\"Mean|median|stdev|min|max for description word length for objects and arrays: \", statistics.mean(non_parameter_descriptions_word_len),\"|\",statistics.median(non_parameter_descriptions_word_len),\"|\",statistics.stdev(non_parameter_descriptions_word_len),\"|\",min(non_parameter_descriptions_word_len),\"|\",max(non_parameter_descriptions_word_len))\n",
    "    print(\"Mean|median|stdev|min|max for description token length for parameters: \", statistics.mean(parameter_descriptions_token_len),\"|\",statistics.median(parameter_descriptions_token_len),\"|\",statistics.stdev(parameter_descriptions_token_len),\"|\",min(parameter_descriptions_token_len),\"|\",max(parameter_descriptions_token_len))\n",
    "    print(\"Mean|median|stdev|min|max for description word length for parameters: \", statistics.mean(parameter_descriptions_word_len),\"|\",statistics.median(parameter_descriptions_word_len),\"|\",statistics.stdev(parameter_descriptions_word_len),\"|\",min(parameter_descriptions_word_len),\"|\",max(parameter_descriptions_word_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1efee3-c072-4c1b-9d66-3bc2be565a55",
   "metadata": {},
   "source": [
    "# Create QA-Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70ba46-9a22-40dc-a532-f72c9a48d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnswer:\n",
    "    \"\"\"\n",
    "    Represents a question-answer pair consisting of a unique identifier, a question, its length (number of tokens), the answer of the question, and the character based index of the start of the answer within the context.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    id : str\n",
    "        unique identifier of a question-answer pair\n",
    "    question : str\n",
    "        question text\n",
    "    question_length : int\n",
    "        number of tokens of question text\n",
    "    answer : str\n",
    "        answer text\n",
    "    answer_start : int\n",
    "        position (index) of the first character of the answer text within original context\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    as_dict():\n",
    "        Converts the question-answer pair into a Python dictionary following the structure for QA pairs recommended in https://huggingface.co/course/chapter7/7?fw=pt  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, question, question_length, answer, answer_position_in_index):\n",
    "        \"\"\"\n",
    "        Constructs a question-answer pair having the passed parameters\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        question : str\n",
    "            question text\n",
    "        question_length : int\n",
    "            number of tokens of question text\n",
    "        answer : str\n",
    "            answer text\n",
    "        answer_position_in_index : int\n",
    "            position (index) of the first character of the answer text within original context\n",
    "        \"\"\"\n",
    "        self.id = uuid.uuid4().hex;\n",
    "        self.question = question\n",
    "        self.question_length = question_length\n",
    "        self.answer = answer\n",
    "        self.answer_start = answer_position_in_index\n",
    "  \n",
    "    def as_dict(self):\n",
    "        \"\"\"\n",
    "        Converts this question-answer pair into a Python dictionary following the structure for QA pairs recommended in https://huggingface.co/course/chapter7/7?fw=pt\n",
    "        {\n",
    "            'id': ....,\n",
    "            'question': ....,\n",
    "            'question_length': .....,\n",
    "            'answers':{\n",
    "                'text': [....],\n",
    "                'answer_start : [....]\n",
    "            }\n",
    "        }\n",
    "        Note that 'answers.text' and 'answers.answer_start' both are lists as in classical NL QA a question might have multiple answers. In our case, every question has exactly one answer, thus, one item per list.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Python dictionary\n",
    "        \"\"\"\n",
    "        a = {}\n",
    "        a[\"text\"] = [self.answer]\n",
    "        a[\"answer_start\"] = [self.answer_start]\n",
    "        q = {}\n",
    "        q[\"id\"] = self.id\n",
    "        q[\"question\"] = self.question\n",
    "        q[\"question_length\"] = self.question_length\n",
    "        q[\"answers\"] = a\n",
    "        return q\n",
    "\n",
    "class QuestionAnswerSample:\n",
    "    \"\"\"\n",
    "    Represents a question-answer sample consisting of a unique identifier, a title, a context, and multiple question-answer pairs extracted from the context \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    id : str\n",
    "        unique identifier of the sample\n",
    "    title : str\n",
    "        title of the sample\n",
    "    context : str\n",
    "        context (text)\n",
    "    questionAnswers : [QuestionAnswer]\n",
    "        question-answer pairs extracted from the context \n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    __str__():\n",
    "        Converts the sample including its question-answer pairs into a JSON structure following the structure for QA pairs recommended in https://huggingface.co/course/chapter7/7?fw=pt\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, context, questionAnswers: list, title = None):\n",
    "        \"\"\"\n",
    "        Constructs a question-answer sample having the passed parameters\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        context : str\n",
    "            context (text)\n",
    "        questionAnswers : [QuestionAnswer]\n",
    "            list of question-answer pairs\n",
    "        title : str\n",
    "            optional title of the sample\n",
    "        \"\"\"\n",
    "        self.id = uuid.uuid4().hex;\n",
    "        for qa in questionAnswers:\n",
    "            qa.id = self.id +\"_\"+qa.id\n",
    "\n",
    "        self.title = title;\n",
    "        self.context = context\n",
    "        self.questionAnswers = questionAnswers\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Converts the sample including its question-answer pairs into a JSON structure following the structure for QA pairs recommended in https://huggingface.co/course/chapter7/7?fw=pt\n",
    "        {\n",
    "            \"id\": ....,\n",
    "            \"title\": ....,\n",
    "            \"context\": ....,\n",
    "            \"questions\": [ //see QuestionAnswer.as_dict()]\n",
    "        }\n",
    "        \"\"\"\n",
    "        json_dict = {}\n",
    "        json_dict[\"id\"] = self.id\n",
    "        json_dict[\"title\"] = self.title\n",
    "        json_dict[\"context\"] = self.context\n",
    "        json_dict[\"questions\"] = [x.as_dict() for x in self.questionAnswers]\n",
    "        return json.dumps(json_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837aec8-5a23-4e00-8675-9e65c220ee70",
   "metadata": {},
   "source": [
    "## Methods for Creating Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290479c-62ef-441b-8eb3-61a932c5e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xpaths(node: ApiInterfaceNode):\n",
    "    \"\"\"\n",
    "    Extracts all XPaths from the API sub tree where the passed ApiInterfaceNode is the root of this tree. Note that the root node will not be processed (only its children) and all nodes, except of the root node, must be type of 'property'.\n",
    "    The method returns the list of extracted XPaths.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : ApiInterfaceNode\n",
    "        root node of the API sub tree whose XPaths should be extracted\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List of extracted XPaths\n",
    "    \"\"\"\n",
    "    xpaths = []\n",
    "    for element in node.elements:\n",
    "        # if node is type of array, object or unknown with childs\n",
    "        if element.property_data_type == \"array\" or element.property_data_type == \"object\" or ((element.property_data_type == \"unknown\" or element.property_data_type == None) and len(element.elements) > 0):\n",
    "            xpaths += extract_xpaths(element)\n",
    "            \n",
    "        # if node is type of string, num, int, bool, or unknown without childs \n",
    "        else:\n",
    "            xpaths.append(element.property_xpath)\n",
    "    return xpaths\n",
    "\n",
    "\n",
    "def filter_xpaths(xpaths: list, max_depth: int):\n",
    "    \"\"\"\n",
    "    Removes all XPaths from the passed list that exceed the specified maximum depth and returns the modified list.\n",
    "    Example: 'users.address.street' has a depth of 3.\n",
    "    If 'max_depth' is None, the passed list will be returned without any removal.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xpaths : [str]\n",
    "        List of XPaths (strings)\n",
    "    max_depth : int\n",
    "        maximum depth (can be None)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Modified list of XPaths\n",
    "    \"\"\"\n",
    "    filtered_xpaths = []\n",
    "    for xpath in xpaths:\n",
    "        if max_depth == None or len(xpath.split(\".\")) <= max_depth:\n",
    "            filtered_xpaths.append(xpath)\n",
    "    return filtered_xpaths\n",
    "\n",
    "\n",
    "def build_context_string(xpaths: list, sort_by_name: bool = False, shuffle: bool = False):\n",
    "    \"\"\"\n",
    "    Removes duplicate XPaths from the passed XPath list, optionally sort (ascending order) or shuffle the list and finally concatenates the remaining XPath items to a string with spaces as speparator between items.\n",
    "    The method returns this resulting string. The returned string is empty, i.e., '', if the list of XPaths is empty.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xpaths : [string]\n",
    "        list of XPaths\n",
    "    sort_by_name : bool\n",
    "        if set to True (default value is False), the method will sort the list of XPath items in ascending order\n",
    "    shuffle : bool\n",
    "        if set to True (default value is False), the method will shuffle the list of XPath items\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    String containing the concatenated XPath items\n",
    "    \"\"\"\n",
    "    \n",
    "    assert not shuffle and sort_by_name, \"Cannot shuffle and sort xpaths at the same time\"\n",
    "    \n",
    "    xpaths_without_duplicate_items = []\n",
    "    deduplication_set = set()\n",
    "    \n",
    "    for xpath in xpaths:\n",
    "        if not xpath in deduplication_set:\n",
    "            xpaths_without_duplicate_items.append(xpath)\n",
    "            deduplication_set.add(xpath)\n",
    "            \n",
    "    # shuffle (if enabled)\n",
    "    if shuffle:\n",
    "        random.shuffle(xpaths_without_duplicate_items)\n",
    "        \n",
    "    # sort by name (if enabled)\n",
    "    if sort_by_name:\n",
    "        xpaths_without_duplicate_items.sort()\n",
    "    \n",
    "    return \" \".join(xpaths_without_duplicate_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f35ef3-11e6-4508-8738-fbe5005f22c9",
   "metadata": {},
   "source": [
    "## Methods for Creating Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b915c-50d2-4dc2-a4f3-86559855eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_inline_uris(text: str):\n",
    "    \"\"\"\n",
    "    Removes URIs from the passed string and returns the modified string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        input string that should be scanned for URIs\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Modified string\n",
    "    \"\"\"\n",
    "    tokens = text.split(' ')\n",
    "    for token in tokens:\n",
    "        if \"http:\" in token.lower() or \"https:\" in token.lower():\n",
    "            text = text.replace(token,\" \")\n",
    "    return text\n",
    "\n",
    "def truncate_question(question: str, max_question_length: int):\n",
    "    \"\"\"\n",
    "    Truncates the passed question (string) if the number of tokens exceeds the passed maximum question length. If the passed question is too long, the method tries to split it into sentences and concatenates these sentences\n",
    "    until maximum question length is exceeded. The method returns the truncated question, its length (number of tokens), and the whether it has been  truncated (True) or not (False)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "        Original question that should be truncated\n",
    "    max_question_length : int\n",
    "        Maximum number of tokens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The truncated question, its length, and whether it has been truncated (True) or not (False)\n",
    "    \n",
    "    \"\"\"\n",
    "    # determine length of original question\n",
    "    length = get_length(question)\n",
    "    \n",
    "    # if question length (number of tokens determined by tokenizer) does not exceed max. question length\n",
    "    if length <= max_question_length:\n",
    "        # return question without any modifications\n",
    "        return question, length, False\n",
    "    \n",
    "    # if question length exceeds max. question length\n",
    "    else:\n",
    "        # (Try to) split question into sentences\n",
    "        sentences = question.split(\".\")\n",
    "        \n",
    "        # calculate length for each sentende \n",
    "        sentences_and_lengths = [(x,get_length(x)) for x in sentences] # generates a tuple of (sentence,length) for each sentence\n",
    "        \n",
    "        # reset question and length\n",
    "        question = \"\"\n",
    "        length = 0\n",
    "        \n",
    "        # concatenate sentences until max. question length is reached\n",
    "        for sentence in sentences_and_lengths:\n",
    "            sentence_text = sentence[0]\n",
    "            sentence_length = sentence[1]\n",
    "            \n",
    "            # if sentence is not empty and new total length does not exceed max. question length\n",
    "            if sentence_text is not None and length + sentence_length < max_question_length:\n",
    "                question = question + sentence_text+\".\"\n",
    "                length = length + sentence_length + 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        return question, get_length(question), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e4a84-4b10-4448-8475-37517eb38c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test truncate_question\n",
    "question = \"This is a short example. We want to test, whether truncate_question works as expected. Hello World\"\n",
    "truncate_question(question,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f4a28-08b3-40b0-8a9c-3f561bb6a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_length(\"This is a short\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e2a2c-5afb-40d8-a25c-71255f6b3c9d",
   "metadata": {},
   "source": [
    "## Methods for Creating Question-Answer Pairs and Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b31e2-f12a-4d2e-b690-f76517d80c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_start(context: str,answer_text: str):\n",
    "    \"\"\"\n",
    "    Returns the position of the first character of the passed answer text in the specified context or None if the answer is not in the context.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    context : str\n",
    "        context\n",
    "    answer_text:\n",
    "        answer text\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Position of the first character of the answer text or None if the answer is not in the context\n",
    "    \"\"\"\n",
    "    \n",
    "    position = 0\n",
    "    for property in context.split():\n",
    "        if answer_text == property:\n",
    "            return position\n",
    "        else:\n",
    "            position += len(property)\n",
    "        position+=1 # for each space\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed336e23-f372-4cd5-9f19-11602bae5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test get_answer_start\n",
    "context = \"user.address user.address.street user.address.city user\"\n",
    "get_answer_start(context,\"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac936c3-e141-4cd6-baac-007f064c8992",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,char in enumerate(context):\n",
    "    print(i,\" \",char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ed41e-0679-4d7c-b270-723a0d33ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question_answer_pairs(node: ApiInterfaceNode, context: str, min_question_length: int = None , max_question_length: int = None, remove_uris: bool = False, max_depth: int = None):\n",
    "    \"\"\"\n",
    "    Create and returns a list of Question-Answer pairs deduced from the passed API sub tree.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    context : str\n",
    "        context, required for calculating the position (character-based index) of the answer\n",
    "    min_question_length : int\n",
    "        Optional parameter (default is None) that specifies the minimum length (number of tokens) that a question must have. If the parameter is None, the minimum length is one token\n",
    "    max_question_length : int\n",
    "        Optional parameter (default is None) that specifies the maximum length (number of tokens) that a question may have. If the parameter is None, the maximum length is unlimited\n",
    "    remove_uris : bool\n",
    "        Optional parameter (default is False) indicating whether URIs should be removed from questions\n",
    "    max_depth : int\n",
    "        Optional parameter (default is None) that specifies the maximum depth that an answer (i.e. the XPath without root element) may have. If the parameter is None, there is no depth limitation\n",
    "        \n",
    "    Results\n",
    "    -------\n",
    "    List of created Question-Answer pairs\n",
    "    \"\"\"\n",
    "    question_answer_pairs = []\n",
    "    \n",
    "    valid = True\n",
    "    \n",
    "    # (changes in v3.1)\n",
    "    global cnt_properties\n",
    "    cnt_properties+=1\n",
    "    \n",
    "    # check whether property is type of string, number, integer, boolean, or unkown whithout children\n",
    "    if (node.property_data_type == \"string\" \n",
    "        or node.property_data_type == \"number\" \n",
    "        or node.property_data_type == \"integer\" \n",
    "        or node.property_data_type == \"boolean\"\n",
    "        or (node.property_data_type == \"unknown\" and len(node.elements) == 0)):\n",
    "        \n",
    "        global cnt_parameters\n",
    "        cnt_parameters+=1\n",
    "        \n",
    "        description = node.property_description\n",
    "        \n",
    "        # check whether description is empty\n",
    "        if not description:\n",
    "            global cnt_parameters_without_descriptions\n",
    "            cnt_parameters_without_descriptions += 1 \n",
    "            \n",
    "        # check whether description consists of at least X tokens (only if min. question length is enabled and description is not empty):\n",
    "        if min_question_length and description: \n",
    "            length = get_length(description)\n",
    "            if length < min_question_length:\n",
    "                global cnt_parameters_with_too_short_descriptions\n",
    "                cnt_parameters_with_too_short_descriptions+=1\n",
    "                # set description to none in order to skip the following steps\n",
    "                description = None\n",
    "                \n",
    "        # continue only if there is a description\n",
    "        if description:\n",
    "    \n",
    "            # Step 1 (optional): remove inline URIs:\n",
    "            if remove_uris:\n",
    "                description = remove_inline_uris(description)\n",
    "                \n",
    "            # Step 2: remove unecessary whitespaces\n",
    "            while '  ' in description:\n",
    "                description = description.replace('  ',' ')\n",
    "            \n",
    "            # Step 3 (optional): truncate description; this step ensures that the length of the returned description does not exceed the maximum length by truncating it.\n",
    "            # Therefore truncate_question might return an empty question, i.e., question with len = 0, if the question cannot be truncated\n",
    "            if max_question_length is not None:\n",
    "                description, length, truncated = truncate_question(description, max_question_length)\n",
    "                if truncated:\n",
    "                    global cnt_parameters_with_truncated_descriptions\n",
    "                    cnt_parameters_with_truncated_descriptions += 1\n",
    "            else:\n",
    "                length = get_length(description)\n",
    "            \n",
    "            # Step 4: again, check whether description contains enough tokens after truncation\n",
    "            if min_question_length:\n",
    "                min_len = min_question_length\n",
    "            else:\n",
    "                min_len = 1\n",
    "                \n",
    "            if length < min_len:\n",
    "                # The description could not be truncated means that:\n",
    "                # 1.) The description is still too long, even after removing trailing sentences (i.e., truncate_question returns an empty description)\n",
    "                # 2.) The description could be truncated, but the resultung description does not contain enough tokens\n",
    "                global cnt_parameters_with_descriptions_that_could_not_be_truncated \n",
    "                cnt_parameters_with_descriptions_that_could_not_be_truncated += 1\n",
    "                valid = False\n",
    "        else:\n",
    "            valid = False\n",
    "        \n",
    "        # count overall issues with description\n",
    "        if not valid:\n",
    "            global cnt_parameter_with_description_constraint_violation\n",
    "            cnt_parameter_with_description_constraint_violation+=1\n",
    "        \n",
    "        # check whether XPath exceeds max. depth\n",
    "        if max_depth is not None and len(node.property_xpath.split(\".\")) > max_depth:\n",
    "            global cnt_parameters_with_too_deep_xpath \n",
    "            cnt_parameters_with_too_deep_xpath += 1\n",
    "            valid = False\n",
    "        \n",
    "        # if all constraints are satisfied \n",
    "        if valid:\n",
    "            \n",
    "            # Build answer:\n",
    "            answer = node.property_xpath\n",
    "            \n",
    "            # Calculate answer position in context\n",
    "            answer_start = get_answer_start(context,answer)\n",
    "            \n",
    "            assert answer_start is not None, \"Answer '\"+answer+\"' of question generated from ID '\"+node.id+\"' is not in context (schema)\"\n",
    "            \n",
    "            # Create Question-Answer pair and add it to list\n",
    "            question_answer = QuestionAnswer(description,length,answer,answer_start)\n",
    "            question_answer_pairs.append(question_answer)\n",
    "            \n",
    "            # if answer is in the context\n",
    "            '''\n",
    "            if answer_start != None:\n",
    "                question_answer = QuestionAnswer(description,length,answer,answer_start)\n",
    "                question_answer_pairs.append(question_answer)\n",
    "            else:\n",
    "                global cnt_answer_not_in_context\n",
    "                cnt_answer_not_in_context += 1\n",
    "                print(\"Error: Answer not in context\")\n",
    "                print(\"ID: \",node.id)\n",
    "                print(\"Context: \",context)\n",
    "                print(\"Answer: \", answer)\n",
    "            '''\n",
    "        else:\n",
    "            global cnt_invalid_parameters\n",
    "            cnt_invalid_parameters+=1\n",
    "    \n",
    "    \n",
    "    # recursive call if node is type of array, object, or unknown with childs\n",
    "    if node.property_data_type == \"array\" or node.property_data_type == \"object\" or ((node.property_data_type == \"unknown\" or node.property_data_type == None) and len(node.elements) > 0):\n",
    "        for element in node.elements:\n",
    "            question_answer_pairs += create_question_answer_pairs(element,context,min_question_length,max_question_length, remove_uris, max_depth)\n",
    "    \n",
    "    return question_answer_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e1a399-fcd9-4aea-9e60-35d525162259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question_answer_samples_for_payload(schema_root_node: ApiInterfaceNode, min_question_length: int = None , max_question_length: int = None, remove_uris: bool = False, max_depth: int = None, max_questions_per_sample: int = None, sort_by_name: bool = False, shuffle_context: bool = False):\n",
    "    \"\"\"\n",
    "    Creates and returns one or multiple Question-answer samples for the passed schema root node. The decision whether one or multiple samples are created depends on the 'max_question_per_sample' threshold as well as the size (number of properties) of the payload.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    schema_root_node : ApiInterfaceNode\n",
    "        Root node of the schema ($)\n",
    "    min_question_length : int\n",
    "        Optional parameter (default is None) that specifies the minimum length (number of tokens) that a question must have. If the parameter is None, the minimum length is one token\n",
    "    max_question_length : int\n",
    "        Optional parameter (default is None) that specifies the maximum length (number of tokens) that a question may have. If the parameter is None, the maximum length is unlimited\n",
    "    remove_uris : bool\n",
    "        Optional parameter (default is False) indicating whether URIs should be removed from questions\n",
    "    max_depth : int\n",
    "        Optional parameter (default is None) that specifies the maximum depth that an XPath (as answer as well as in context) may have. If the parameter is None, there is no depth limitation\n",
    "    max_questions_per_sample : int\n",
    "        Optional parameter (default is None) that specifies the maximum number of Question-Answer pairs per sample. The method distributes the Question-Answer pairs to mulitple samples if this limit is exceeded.\n",
    "    sort_by_name : bool\n",
    "        if set to True (default value is False), the method will sort XPaths in context in ascending order\n",
    "    shuffle : bool\n",
    "        if set to True (default value is False), the method will shuffle XPaths in context\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List of created Question-Answer samples (even if one sample is created, it is a list)\n",
    "    \"\"\"\n",
    "    # Build context\n",
    "    xpaths = extract_xpaths(schema_root_node)\n",
    "    if max_depth:\n",
    "        xpaths = filter_xpaths(xpaths,max_depth)\n",
    "    context = build_context_string(xpaths,sort_by_name,shuffle_context)\n",
    "    \n",
    "    # Warning: context might be empty (i.e., '') if the payload does not contain any parameter or all parameter XPaths exceed the maximum depth.\n",
    "    # However, the following steps in create_question_answer_pairs will not create any question-answer pairs for these empty schemas (context), since the XPath depth will be verified again for each parameter.\n",
    "    # Nevertheless, we process any parameter, even if the context is empty, in order to collect all metrics about description length, etc.\n",
    "    \n",
    "    question_answer_pairs = create_question_answer_pairs(schema_root_node, context, min_question_length, max_question_length, remove_uris, max_depth)\n",
    "\n",
    "    # Check if at least one Question-Answer pair has been created:\n",
    "    if question_answer_pairs:\n",
    "        global cnt_payload_with_samples\n",
    "        cnt_payload_with_samples +=1\n",
    "        \n",
    "        if max_questions_per_sample is not None:\n",
    "            samples = []\n",
    "\n",
    "            while question_answer_pairs:\n",
    "                counter = 0\n",
    "                partial_question_answer_pairs = []\n",
    "                while len(question_answer_pairs) > 0 and counter < max_questions_per_sample:\n",
    "                    partial_question_answer_pairs.append(question_answer_pairs.pop())\n",
    "                    counter += 1\n",
    "                sample = QuestionAnswerSample(context, partial_question_answer_pairs, schema_root_node.id)\n",
    "                samples.append(sample)\n",
    "\n",
    "            if len(samples) > 1:\n",
    "                global cnt_split_samples\n",
    "                cnt_split_samples += 1\n",
    "            return samples\n",
    "        else:\n",
    "            sample = QuestionAnswerSample(context, question_answer_pairs, schema_root_node.id)\n",
    "            return [sample]\n",
    "    else:\n",
    "        global cnt_payload_without_samples\n",
    "        cnt_payload_without_samples += 1\n",
    "        \n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee6602-f062-47ba-9f38-979b98896fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_cnt = 0\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "for api in tqdm(apis):\n",
    "    if int(api.api_key) in excluded_api_keys:\n",
    "        print(\"Skip \",api.api_name,\" (\",api.api_key,\")\")\n",
    "        continue\n",
    "\n",
    "    question_cnt_per_api = 0\n",
    "    payload_cnt_per_api = 0\n",
    "    samples = []\n",
    "\n",
    "    for i in range(original_retakes):\n",
    "        for payload_node in extract_nodes(api,node_type=\"payload\"):\n",
    "            payload_cnt_per_api+=1\n",
    "            if len(payload_node.elements) == 1:\n",
    "                root_node = payload_node.elements[0]\n",
    "                s = create_question_answer_samples_for_payload(root_node,min_question_length,max_question_length,remove_uris,max_depth,max_questions_per_sample, sort_by_name, False)\n",
    "                if s:\n",
    "                    samples+= s\n",
    "            else:\n",
    "                cnt_empty_payloads +=1\n",
    "\n",
    "    for i in range(shuffled_retakes):\n",
    "        for payload_node in extract_nodes(api,node_type=\"payload\"):\n",
    "            payload_cnt_per_api+=1\n",
    "            if len(payload_node.elements) == 1:\n",
    "                root_node = payload_node.elements[0]\n",
    "                s = create_question_answer_samples_for_payload(root_node,min_question_length,max_question_length,remove_uris,max_depth,max_questions_per_sample, False, True)\n",
    "                if s:\n",
    "                    samples+= s\n",
    "            else:\n",
    "                cnt_empty_payloads +=1\n",
    "\n",
    "    if samples:\n",
    "        for sample in samples:\n",
    "            question_cnt_per_api += len(sample.questionAnswers)\n",
    "            question_cnt += len(sample.questionAnswers)\n",
    "        results.append({\n",
    "            \"samples\":samples,\n",
    "            \"api_key\":api.api_key,\n",
    "            \"api_name\":api.api_name,\n",
    "            \"api_version_key\":api.api_version_key,\n",
    "            \"api_version_name\":api.api_version_name,\n",
    "            \"payload_cnt_per_api\":payload_cnt_per_api,\n",
    "            \"question_cnt_per_api\":question_cnt_per_api\n",
    "        })\n",
    "        \n",
    "            \n",
    "            \n",
    "       \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7273a9-841d-4714-bf09-39a2e0543c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = sorted(results, key=lambda item: item[\"question_cnt_per_api\"],reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a8bd49-38ce-4da1-b360-4df3c9456371",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [[] for i in range(number_of_chunks)]\n",
    "\n",
    "with open(output_path+datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")+\".log.csv\",\"w\") as log_file:\n",
    "    log_file.write(\"API Key;API Name;API Version Key; API Version;#Payloads;#Samples;#Questions;Out File\\n\")\n",
    "    for result in sorted_results:\n",
    "        smallest_chunk_index = 0\n",
    "        smallest_chunk_size = None\n",
    "        for i in range(number_of_chunks):\n",
    "            num_questions = 0\n",
    "            for sample in chunks[i]:\n",
    "                num_questions+=result[\"question_cnt_per_api\"]\n",
    "            if smallest_chunk_size == None or num_questions < smallest_chunk_size:\n",
    "                smallest_chunk_size = num_questions\n",
    "                smallest_chunk_index = i\n",
    "        chunks[smallest_chunk_index]+= result[\"samples\"]\n",
    "        filename = str(smallest_chunk_index)+\".json\"\n",
    "\n",
    "        log_file.write(str(result[\"api_key\"])+\";\"+str(result[\"api_name\"])+\";\"+str(result[\"api_version_key\"])+\";\"+str(result[\"api_version_name\"])+\";\"+str(result[\"payload_cnt_per_api\"])+\";\"+str(len(result[\"samples\"]))+\";\"+str(result[\"question_cnt_per_api\"])+\";\"+filename+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6bb44-8bc7-4742-a629-aa1f635851c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of processed properties\n",
    "print(\"# Properties: \",cnt_properties)\n",
    "\n",
    "# number of processed paramters, i.e., properties that have a primitive type and, therefore, are candidates for question-answer pairs\n",
    "print(\"# Parameters: \",cnt_parameters)\n",
    "\n",
    "# number of parameters that do not satisfy the contraints required to generate a question-answer pair\n",
    "print(\"# Invalid Parameters: \",cnt_invalid_parameters)\n",
    "\n",
    "# number of generated question-answer pairs, which is equal to the number of paramters that satisfy all contraints for being a question-answer pair\n",
    "print(\"# Questions: \",question_cnt)\n",
    "assert question_cnt == cnt_parameters-cnt_invalid_parameters, \"Mismatch between number of generated question-answer pairs and invalid parameters\"\n",
    "\n",
    "# number of payloads from which at least one question-answer pair can be generated\n",
    "print(\"# Payloads with at least one QA sample: \",cnt_payload_with_samples);\n",
    "# number of payloads from which no question-answer pairs can be generated, since the payload does not contain any parameters that satisfies the constraints\n",
    "print(\"# Payloads without any QA samples: \", cnt_payload_without_samples)\n",
    "#  number of payloads that do not contain one element (i.e., can be considered as empty)\n",
    "print(\"# Empty payloads: \", cnt_empty_payloads)\n",
    "\n",
    "# (added in v3.2) number of parameters whose descriptions do not satisfy at least one constraint\n",
    "print(\"# Parameters with description violation: \", cnt_parameter_with_description_constraint_violation)\n",
    "# number of parameters that must be excluded due to a missing description\n",
    "print(\"# Parameters without descriptions: \", cnt_parameters_without_descriptions)\n",
    "# number of parameters whose XPath exceeds the maximum depth\n",
    "print(\"# Parameters with too deep XPaths: \", cnt_parameters_with_too_deep_xpath)\n",
    "# number of paramters whose descriptions are too short\n",
    "print(\"# Parameters with too short descriptions (even before truncation): \", cnt_parameters_with_too_short_descriptions)\n",
    "\n",
    "# number of parameters whose description cannot be truncated, since:\n",
    "# 1.) The description is still too long, even after removing trailing sentences (i.e., truncate_question returns an empty description)\n",
    "# 2.) The description could be truncated, but the resultung description does not contain enough tokens\n",
    "print(\"# Properties with descriptions could not be truncated (because they were too long or too short after truncation): \", cnt_parameters_with_descriptions_that_could_not_be_truncated)\n",
    "\n",
    "assert cnt_invalid_parameters <= cnt_parameters_without_descriptions+cnt_parameters_with_too_deep_xpath+cnt_parameters_with_too_short_descriptions+cnt_parameters_with_descriptions_that_could_not_be_truncated, \"Mismatch between number of invalid parameters and numbers specifying reasons for being invalid\"\n",
    "\n",
    "# parameters with descriptions that are truncated\n",
    "print(\"# Properties with descriptions that could be truncated: \", cnt_parameters_with_truncated_descriptions)\n",
    "\n",
    "\n",
    "print(\"# Original samples that have been split into multiple samples: \", cnt_split_samples)\n",
    "print(\"# Errors: Answer not in context: \", cnt_answer_not_in_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05159794-7b3b-4833-84de-5de9e748b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle samples\n",
    "for i in range(number_of_chunks):\n",
    "    random.shuffle(chunks[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e254d1b-cd01-4ca2-ba8a-21385a1abbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print chunks\n",
    "for i in range(number_of_chunks):\n",
    "    q_cnt = 0\n",
    "    for sample in chunks[i]:\n",
    "            q_cnt += len(sample.questionAnswers)\n",
    "    print(i,\": \",len(chunks[i]),\" samples / \",q_cnt,\" questions (\", (q_cnt/question_cnt)*100,\"%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dea1810-91ae-48b7-a1f9-15f37dc4565b",
   "metadata": {},
   "source": [
    "# Analyze Question and Paragraph Length (added in v3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674bbe15-aad1-44dc-a520-33ee804b3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_word_len = []\n",
    "question_token_len = []\n",
    "parameter_xpath_len = []\n",
    "parameters_per_schema = []\n",
    "schema_token_len = []\n",
    "\n",
    "unique_schema_set = set()\n",
    "\n",
    "for i in range(number_of_chunks):\n",
    "    for sample in tqdm(chunks[i]):\n",
    "        token_s_len = get_length(sample.context)\n",
    "        for question in sample.questionAnswers:\n",
    "            word_q_len = len(word_tokenize(question.question))\n",
    "            token_q_len = get_length(question.question)\n",
    "            \n",
    "            #if word_q_len == 117:\n",
    "            #    print(question.question)\n",
    "            \n",
    "            question_word_len.append(word_q_len)\n",
    "            question_token_len.append(token_q_len)\n",
    "            parameters_per_schema.append(len(sample.context.split(\" \")))\n",
    "            schema_token_len.append(token_s_len)\n",
    "            \n",
    "            if sample.context not in unique_schema_set:\n",
    "                 unique_schema_set.add(sample.context)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a87c5-4081-4650-b151-15360f4e5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean|median|stdev|min|max for question token length: \", statistics.mean(question_token_len),\"|\",statistics.median(question_token_len),\"|\",statistics.stdev(question_token_len),\"|\",min(question_token_len),\"|\",max(question_token_len))\n",
    "print(\"Mean|median|stdev|min|max for question word length: \", statistics.mean(question_word_len),\"|\",statistics.median(question_word_len),\"|\",statistics.stdev(question_word_len),\"|\",min(question_word_len),\"|\",max(question_word_len))\n",
    "print(\"Mean|median|stdev|min|max for number of parameters per schema: \", statistics.mean(parameters_per_schema),\"|\",statistics.median(parameters_per_schema),\"|\",statistics.stdev(parameters_per_schema),\"|\",min(parameters_per_schema),\"|\",max(parameters_per_schema))\n",
    "print(\"Mean|median|stdev|min|max for schema token length: \", statistics.mean(schema_token_len),\"|\",statistics.median(schema_token_len),\"|\",statistics.stdev(schema_token_len),\"|\",min(schema_token_len),\"|\",max(schema_token_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad3aa7-36be-4efa-ab4b-618183aedd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(parameters_per_schema, color = 'blue', edgecolor = 'black',\n",
    "         bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba97062-4d22-4c67-83b9-8d937f4432e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_context_token_len = []\n",
    "parameters_per_unique_context = []\n",
    "\n",
    "for context in tqdm(unique_schema_set):\n",
    "    context_len = get_length(context)\n",
    "    unique_context_token_len.append(context_len)\n",
    "    parameters_per_unique_context.append(len(context.split(\" \")))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000bc4bc-29e4-4ecf-97bd-0a30999dbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique schemas: \",len(unique_schema_set))\n",
    "print(sum(parameters_per_unique_context))\n",
    "print(\"Mean|median|stdev|min|max for schema token length: \", statistics.mean(unique_context_token_len),\"|\",statistics.median(unique_context_token_len),\"|\",statistics.stdev(unique_context_token_len),\"|\",min(unique_context_token_len),\"|\",max(unique_context_token_len))\n",
    "print(\"Mean|median|stdev|min|max for number of parameters per schema: \", statistics.mean(parameters_per_unique_context),\"|\",statistics.median(parameters_per_unique_context),\"|\",statistics.stdev(parameters_per_unique_context),\"|\",min(parameters_per_unique_context),\"|\",max(parameters_per_unique_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84e5bb-b8bd-49c7-9204-6d19cc42a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(parameters_per_unique_context, color = 'blue', edgecolor = 'black',\n",
    "         bins = 100, range=(1000,7000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1772ede-6b4f-4226-82a7-f461b9bdef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write chunks\n",
    "for i in range(number_of_chunks):\n",
    "  with open(output_path+str(i)+\".json\",\"w\") as file:\n",
    "    for sample in chunks[i]:\n",
    "      file.write(str(sample))\n",
    "      file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1afabe-6402-4991-8acb-f207ab8000f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write validation set\n",
    "validation_index = 2\n",
    "with open(output_path+\"validation.json\",\"w\") as file:\n",
    "    for sample in chunks[validation_index]:\n",
    "        file.write(str(sample))\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743001a-8ddc-442e-b4c2-5af4dfc6a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write test set\n",
    "test_index = 9\n",
    "with open(output_path+\"test.json\",\"w\") as file:\n",
    "    for sample in chunks[test_index]:\n",
    "        file.write(str(sample))\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194b81b-942b-47a7-8f5c-a0090ea017ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = [0,1,3,4,5,6,7,8]\n",
    "train_samples = []\n",
    "for i in train_indices:\n",
    "    train_samples += chunks[i]\n",
    "random.shuffle(train_samples)\n",
    "\n",
    "with open(output_path+\"train.json\",\"w\") as file:\n",
    "    for sample in train_samples:\n",
    "        file.write(str(sample))\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca77ed7-bfe8-4ca2-bbd9-a7c6eecc3d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hf]",
   "language": "python",
   "name": "conda-env-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
