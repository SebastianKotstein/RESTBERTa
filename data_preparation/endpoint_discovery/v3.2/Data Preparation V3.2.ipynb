{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7aeded-40de-476f-a82f-c6f9e205965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes (v3.1):\n",
    "# - We added and renamed several counters (metrics)\n",
    "# - Bug-Fix: In v3, the script counts all properties with too deep paths and having no descriptions, but only parameters, i.e., properties being a leaf, should be considered. \n",
    "# - We removed the verification whether the context is empty in create_question_answer_samples_for_payload\n",
    "\n",
    "# Changes (v3.2):\n",
    "# - We analyse the description length of all schema properties\n",
    "# - As well as the description length in the final QA samples and the length in the schema and the number of parameters per schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db764115-9c86-404c-bda7-536740ef8768",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46202d2d-73a7-43c3-872d-eecc5ddef256",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d24e41-6ef1-4fdb-8964-f65b1b8e2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ac28f-6a2a-46f7-836a-ae051ee810a6",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76bfa97-ed47-4252-81a4-0328362a00c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import uuid\n",
    "import operator\n",
    "import os\n",
    "\n",
    "# tqdm is used to visualize the progress while processing input files\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for embedding current time into log file name \n",
    "from datetime import datetime\n",
    "\n",
    "# We will use a pre-trained tokenizer to determine the length of strings\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# for description lenght analysis (added in v3.2)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146f744-113e-4727-8cf3-03c94b468160",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0df4f-fa6a-4149-95fa-ed3849cdc94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if set, only the first 'n' API tree models will be loaded\n",
    "api_limit = None\n",
    "\n",
    "remove_uris = True  # remove URIs from description\n",
    "sort_by_name = True # sort context by name\n",
    "\n",
    "max_depth = 8 # max. depth of XPath in both context and answer\n",
    "min_question_length = 3 # min. number of tokens that must be in a question\n",
    "max_question_length = 96 # max. number of tokens that may be in a question\n",
    "\n",
    "max_questions_per_sample = 32 # max. number of Question-Answer pairs per sample. If number is exceeded, additional sample is created\n",
    "\n",
    "number_of_chunks = 10 # number of containers where samples are distributed to\n",
    "\n",
    "# Variable that specifies how many times the generated sample set is repeated. A value of '1' means that each sample is only created once.\n",
    "original_retakes = 1\n",
    "shuffled_retakes = 0\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Specify the base model that is used for training later. We will use its pre-trained tokenizer in this notebook.\n",
    "base_model = \"microsoft/codebert-base\"\n",
    "\n",
    "# List of special tokens that should be removed from XPaths while creating context string\n",
    "to_be_removed = [\"<?>\",\"<str>\",\"<num>\",\"<int>\",\"<bool>\",\"{_}\",\"$.\"]\n",
    "\n",
    "input_path = \"/home/user/input_directory/\"\n",
    "output_path = \"/home/user/output_directory/\"\n",
    "\n",
    "# APIs (identified by their keys) that should be excluded from processing after loading and parsing them (e.g. due to too many payloads)\n",
    "excluded_api_keys = []\n",
    "\n",
    "http_verb_position = \"suffix\"\n",
    "remove_path_parameter_indicator = False\n",
    "remove_path_fragment_indicator = False\n",
    "\n",
    "# (added in v3.2)\n",
    "pre_description_length_analysis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807b27b-ad26-43d2-9a6f-7b3bb1d61c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of processed operations\n",
    "cnt_operations = 0\n",
    "# number of operations that do not satisfy the contraints required to generate a question-answer pair\n",
    "cnt_invalid_operations = 0\n",
    "\n",
    "# (added in v3.2) number of operations whose descriptions do not satisfy at least one constraint\n",
    "cnt_operation_with_description_constraint_violation = 0\n",
    "# number of operations that must be excluded due to missing descriptions\n",
    "cnt_operations_without_descriptions = 0\n",
    "# number of operations whose path exceeds the maximum depth\n",
    "cnt_operations_with_too_deep_path = 0\n",
    "# number of operations whose description are too short\n",
    "cnt_operations_with_too_short_descriptions = 0\n",
    "# number of operations whose description cannot be truncated, since:\n",
    "# 1.) The description is still too long, even after removing trailing sentences (i.e., truncate_question returns an empty description)\n",
    "# 2.) The description could be truncated, but the resultung description does not contain enough tokens\n",
    "cnt_operations_with_descriptions_that_could_not_be_truncated = 0 \n",
    "\n",
    "# operations with descriptions that are truncated\n",
    "cnt_operations_with_truncated_descriptions = 0\n",
    "\n",
    "cnt_split_samples = 0\n",
    "\n",
    "# apis without any QA sample\n",
    "cnt_apis_without_samples = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8570ec-cf40-49a7-b65e-e1c4f6c44914",
   "metadata": {},
   "source": [
    "# Load and Parse API Tree Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a423970-e75f-43c2-9956-a9011f131f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_data_types_from_xpath(xpath: str):\n",
    "    \"\"\"\n",
    "    Removes special tokens defined in 'to_be_removed' from the passed XPath (input string) and returns the modified string\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xpath : str\n",
    "        input string\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Modified input string\n",
    "    \"\"\"\n",
    "    for data_type in to_be_removed:\n",
    "        xpath = xpath.replace(data_type,\"\")\n",
    "    return xpath\n",
    "\n",
    "\n",
    "class ApiInterfaceNode:\n",
    "    \"\"\"\n",
    "    Represents a generic node in an API tree.\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    key : str\n",
    "        key that uniquely identifies the node among all children of the parent node\n",
    "    value\n",
    "        optional value of the node\n",
    "    node_type: str\n",
    "        type (i.e. role) of the node, allowed values are 'api', 'path', 'method', 'response', 'payload', and 'property'\n",
    "    id : str\n",
    "        unique identifier of the node among all nodes of the API tree\n",
    "    elements : ApiInterfaceNode\n",
    "        contains all children of the node\n",
    "    parent : ApiInterfaceNode\n",
    "        contains the parent of this node\n",
    "    raw_node\n",
    "        contains the original JSON structure of the node and the sub tree as Python dictionary\n",
    "    api_key : str\n",
    "        contains the API key, this attribute is only present if node is type of 'api'\n",
    "    api_name: str\n",
    "        contains the API name, this attribute is only present if node is type of 'api'\n",
    "    api_version_key : str\n",
    "        contains the API version key, this attribute is only present if node is type of 'api'\n",
    "    api_version_name : str\n",
    "        contains the API version name, this attribute is only present if node is type of 'api'\n",
    "    method_summary : str\n",
    "        contains the summary of the method, this attribute is only present if node is type of 'method'\n",
    "    method_description : str\n",
    "        contains the description of the method, this attribute is only present if node is type of 'method'\n",
    "    response_description : str\n",
    "        contains the description of the response, this attribute is only present if node is type of 'response'\n",
    "    property_name : str\n",
    "        contains the name of the property, this attribute is only present if node is type of 'property'\n",
    "    property_data_type : str\n",
    "        contains the data type of the property, this attribute is only present if node is type of 'property'\n",
    "    property_xpath : str\n",
    "        contains the XPath of the property, this attribute is only present if node is type of 'property'\n",
    "    property_format : str\n",
    "        contains the format of the property, this attribute is only present if node is type of 'property'\n",
    "    property_pattern : str\n",
    "        contains the pattern of the property, this attribute is only present if node is type of 'property'\n",
    "    property_description : str\n",
    "        contains the description of the property, this attribute is only present if node is type of 'property'\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    is_type(node_type : str):\n",
    "        Returns true if the node's type is equal the passed type\n",
    "    __str__(): \n",
    "        Returns a JSON object as string containing all attributes of the node\n",
    "    \"\"\"\n",
    "    def __init__(self, api_documentation_raw_node, parent_node):\n",
    "        \"\"\"\n",
    "        Constructs the sub tree consisting of ApiInterfaceNodes based on the passed raw API tree model (parsed JSON structrure as Python dictionary).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        api_documentation_raw_node\n",
    "            parsed JSON structure of the API tree model as Python dictionary\n",
    "        \"\"\"\n",
    "        self.raw_node = api_documentation_raw_node\n",
    "\n",
    "        # Generic attributes\n",
    "        self.key = api_documentation_raw_node[\"key\"]\n",
    "        self.value = api_documentation_raw_node[\"value\"]\n",
    "        self.node_type = api_documentation_raw_node[\"type\"]\n",
    "        self.id = api_documentation_raw_node[\"id\"].replace(\"-\",\".\")\n",
    "        #self.id = parent_id+\".\"+self.key\n",
    "\n",
    "        self.elements = [ApiInterfaceNode(api_documentation_raw_node[\"elements\"][i],self) for i in range(len(api_documentation_raw_node[\"elements\"]))]\n",
    "        self.parent = parent_node\n",
    "        \n",
    "        if self.node_type == \"api\":\n",
    "            self.api_key = api_documentation_raw_node[\"apiKey\"]\n",
    "            self.api_name = api_documentation_raw_node[\"apiName\"]\n",
    "            self.api_version_key = api_documentation_raw_node[\"versionKey\"]\n",
    "            self.api_version_name = api_documentation_raw_node[\"versionName\"]\n",
    "    \n",
    "        if self.node_type == \"pathSegment\":\n",
    "            # no individual attributes for path type\n",
    "            pass\n",
    "\n",
    "        if self.node_type == \"method\":\n",
    "            self.method_summary = api_documentation_raw_node[\"summary\"]\n",
    "            self.method_description = api_documentation_raw_node[\"description\"]\n",
    "\n",
    "        if self.node_type == \"response\":\n",
    "            self.response_description = api_documentation_raw_node[\"description\"]\n",
    "    \n",
    "        if self.node_type == \"payload\":\n",
    "            # no individual attributes for payload type\n",
    "            pass\n",
    "\n",
    "        if self.node_type == \"property\":\n",
    "            self.property_name = api_documentation_raw_node[\"name\"]\n",
    "            self.property_data_type = api_documentation_raw_node[\"dataType\"]\n",
    "            self.property_xpath = remove_data_types_from_xpath(api_documentation_raw_node[\"xpath\"].replace(' ','').replace('\\t','').replace('\\n',''))\n",
    "            self.property_format = api_documentation_raw_node[\"format\"]\n",
    "            self.property_pattern = api_documentation_raw_node[\"pattern\"]\n",
    "            self.property_description = api_documentation_raw_node[\"description\"]\n",
    "        \n",
    "    def is_type(self, node_type: str):\n",
    "        \"\"\"\n",
    "        Returns true, if the node's type is equal the passed type, else false.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_type : str\n",
    "            type that should be compared with the type of the node\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        True or False\n",
    "        \"\"\"\n",
    "        return self.node_type == node_type\n",
    "  \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a JSON object as string containing all attributes of the node\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        JSON object as string containing all attributes of the node\n",
    "        \"\"\"\n",
    "        json_dict = {}\n",
    "        json_dict[\"key\"] = self.key\n",
    "        json_dict[\"value\"] = self.value\n",
    "        json_dict[\"id\"] = self.id\n",
    "        json_dict[\"type\"] = self.node_type\n",
    "        json_dict[\"number_of_elements\"] = len(self.elements)\n",
    "\n",
    "        if self.node_type == \"api\":\n",
    "            json_dict[\"apiKey\"] = self.api_key\n",
    "            json_dict[\"apiName\"] = self.api_name\n",
    "            json_dict[\"versionKey\"] = self.api_version_key\n",
    "            json_dict[\"versionName\"] = self.api_version_name\n",
    "    \n",
    "        if self.node_type == \"pathSegment\":\n",
    "            # no individual attributes for path type\n",
    "            pass\n",
    "\n",
    "        if self.node_type == \"method\":\n",
    "            json_dict[\"summary\"] = self.method_summary\n",
    "            json_dict[\"description\"] = self.method_description\n",
    "\n",
    "        if self.node_type == \"response\":\n",
    "            json_dict[\"description\"] = self.response_description\n",
    "    \n",
    "        if self.node_type == \"payload\":\n",
    "            # no individual attributes for payload type\n",
    "            pass\n",
    "\n",
    "        if self.node_type == \"property\":\n",
    "            json_dict[\"name\"] = self.property_name \n",
    "            json_dict[\"dataType\"] = self.property_data_type \n",
    "            json_dict[\"xpath\"] = self.property_xpath \n",
    "            json_dict[\"format\"] = self.property_format\n",
    "            json_dict[\"pattern\"] = self.property_pattern\n",
    "            json_dict[\"description\"] = self.property_description\n",
    "    \n",
    "        return json.dumps(json_dict)\n",
    "\n",
    "def load_and_parse_api(path: str):\n",
    "    \"\"\"\n",
    "    Loads and parses an API tree model file located under the passed path and converts it structure into a structure of ApiInterfaceNodes.\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path of the API tree model file\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    ApiInterfaceNode representing the root of the loaded and parsed API tree model\n",
    "    \"\"\"\n",
    "    with open(path,\"r\",encoding=\"utf-8\") as json_file:\n",
    "        json_api = json.load(json_file)\n",
    "    return ApiInterfaceNode(json_api, None)\n",
    "\n",
    "def load_and_parse_apis_from_directory(path: str, limit: int = None):\n",
    "    \"\"\"\n",
    "    Loads and parses multiple API tree model files located in the specified directory.\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to directory where the API tree model files are located\n",
    "      \n",
    "    limit : int\n",
    "        Optional limit. If specified, only the first 'n' API tree model files are loaded and parsed\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    List of ApiInterfaceNodes where each node represents the root of a loaded and parsed API tree model\n",
    "    \"\"\"\n",
    "    apis = []\n",
    "    if limit:\n",
    "        filesnames = os.listdir(path)[:limit]\n",
    "    else:\n",
    "        filesnames = os.listdir(path)\n",
    "\n",
    "    for filename in tqdm(filesnames):\n",
    "        if filename.endswith(\".json\"):\n",
    "            apis.append(load_and_parse_api(os.path.join(path,filename)))\n",
    "    return apis\n",
    "\n",
    "def extract_nodes(node: ApiInterfaceNode, node_type: str):\n",
    "    \"\"\"\n",
    "    Extracts all nodes matching the passed node type from the passed API tree model.\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : ApiInterfaceNode\n",
    "      API tree model (input)\n",
    "    node_type : str\n",
    "      The type of the node that should be extracted\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of ApiInterfaceNodes matching the passed node type\n",
    "    \"\"\"\n",
    "    nodes = []\n",
    "    if node.node_type == node_type:\n",
    "        nodes.append(node)\n",
    "    for element in node.elements:\n",
    "        nodes += extract_nodes(element, node_type)\n",
    "    return nodes\n",
    "\n",
    "def extract_nodes_in_apis(nodes: [ApiInterfaceNode], node_type: str):\n",
    "    \"\"\"\n",
    "    Extracts all nodes matching the passed node type from the passed list of API tree models.\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : [ApiInterfaceNodes]\n",
    "      List of API tree models (input)\n",
    "    node_type : str\n",
    "      The type of the node that should be extracted\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of ApiInterfaceNodes matching the passed node type\n",
    "    \"\"\"\n",
    "    extracted_nodes = []\n",
    "    for node in nodes:\n",
    "        extracted_nodes += extract_nodes(node,node_type)\n",
    "    return extracted_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a630a1-9c87-4b8f-b9ef-453208c85613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse API tree models\n",
    "apis = load_and_parse_apis_from_directory(input_path,limit=api_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9431fdbf-4f9a-4139-92a7-fcb2e32b428a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Pre-Trained Tokenizer for Length Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39150aa-dac0-4b72-8cbb-4b583fc5a029",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03f9c0-cda7-4453-8b02-21a18f29104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(input: str):\n",
    "    \"\"\"\n",
    "    Calculates and returns the length (i.e. number of tokens) of the passed input string.\n",
    "    The returned length is '0', if the input is 'None'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input : str\n",
    "        Input string whose length should be calculated\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Number of tokens\n",
    "    \"\"\"\n",
    "    if input:\n",
    "        return len(tokenizer.encode(input, add_special_tokens=False))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23448e1-97b6-46f0-9ed9-60d7b71b7758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tokenizer\n",
    "test_string = \"users.{userId}.get\"\n",
    "get_length(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab87cd8-fb86-49c2-8902-60952d1c20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing length limitations\n",
    "s = \" \".join([str(x) for x in range(99999)])\n",
    "get_length(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54980008-3a39-43f7-bf82-610984af0f89",
   "metadata": {},
   "source": [
    "# Description Length Analysis (added in v3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7a01b-7f22-4db1-87c5-4721cc401d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract payload nodes\n",
    "operation_nodes = extract_nodes_in_apis(apis,node_type=\"method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ffcbf-d593-4ecc-bd1d-faa881f2cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of operation nodes: \",len(operation_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e811aa-e4cc-4417-85c9-3d4331fa70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_description_length_analysis:    \n",
    "    num_operations_with_summary = 0\n",
    "    num_operations_with_descriptions = 0\n",
    "    num_operations_with_summary_and_descriptions = 0\n",
    "    \n",
    "    operation_descriptions = []\n",
    "    operation_summaries = [] \n",
    "    \n",
    "    for operation in operation_nodes:\n",
    "        if operation.method_summary and not operation.method_description:\n",
    "            num_operations_with_summary+=1\n",
    "            \n",
    "        if not operation.method_summary and operation.method_description:\n",
    "            num_operations_with_descriptions+=1\n",
    "        \n",
    "        if operation.method_summary and operation.method_description:\n",
    "            num_operations_with_summary_and_descriptions+=1\n",
    "        \n",
    "        if operation.method_summary:\n",
    "            operation_summaries.append(operation.method_summary)\n",
    "            \n",
    "        if operation.method_description:\n",
    "            operation_descriptions.append(operation.method_description)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cee83-1684-4a36-8680-de3fc4a03fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_description_length_analysis: \n",
    "    print(\"Number of operations: \",len(operation_nodes))\n",
    "    print(\"Number of operations with summary only: \",num_operations_with_summary)\n",
    "    print(\"Number of operations with description only: \",num_operations_with_descriptions)\n",
    "    print(\"Number of operations with summary and description: \",num_operations_with_summary_and_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4119263-b144-4770-9613-a92616c3c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_description_length_analysis: \n",
    "    operation_description_word_len = []\n",
    "    operation_description_token_len = []\n",
    "    operation_summary_word_len = []\n",
    "    operation_summary_token_len = []\n",
    "\n",
    "    for description in tqdm(operation_descriptions):\n",
    "        operation_description_token_len.append(get_length(description))\n",
    "        operation_description_word_len.append(len(word_tokenize(description)))\n",
    "\n",
    "    for summary in tqdm(operation_summaries):\n",
    "        operation_summary_token_len.append(get_length(summary))\n",
    "        operation_summary_word_len.append(len(word_tokenize(summary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66c480-14aa-46fa-8093-54cee8228f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_description_length_analysis: \n",
    "    print(\"Mean|median|stdev|min|max for description token length: \", statistics.mean(operation_description_token_len),\"|\",statistics.median(operation_description_token_len),\"|\",statistics.stdev(operation_description_token_len),\"|\",min(operation_description_token_len),\"|\",max(operation_description_token_len))\n",
    "    print(\"Mean|median|stdev|min|max for description word length: \", statistics.mean(operation_description_word_len),\"|\",statistics.median(operation_description_word_len),\"|\",statistics.stdev(operation_description_word_len),\"|\",min(operation_description_word_len),\"|\",max(operation_description_word_len))\n",
    "    print(\"Mean|median|stdev|min|max for summary token length: \", statistics.mean(operation_summary_token_len),\"|\",statistics.median(operation_summary_token_len),\"|\",statistics.stdev(operation_summary_token_len),\"|\",min(operation_summary_token_len),\"|\",max(operation_summary_token_len))\n",
    "    print(\"Mean|median|stdev|min|max for summary word length: \", statistics.mean(operation_summary_word_len),\"|\",statistics.median(operation_summary_word_len),\"|\",statistics.stdev(operation_summary_word_len),\"|\",min(operation_summary_word_len),\"|\",max(operation_summary_word_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1efee3-c072-4c1b-9d66-3bc2be565a55",
   "metadata": {},
   "source": [
    "# Create QA-Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70ba46-9a22-40dc-a532-f72c9a48d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnswer:\n",
    "    \"\"\"\n",
    "    Represents a question-answer pair consisting of a unique identifier, a question, its length (number of tokens), the answer of the question, and the character based index of the start of the answer within the context.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    id : str\n",
    "        unique identifier of a question-answer pair\n",
    "    question : str\n",
    "        question text\n",
    "    question_length : int\n",
    "        number of tokens of question text\n",
    "    answer : str\n",
    "        answer text\n",
    "    answer_start : int\n",
    "        position (index) of the first character of the answer text within original context\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    as_dict():\n",
    "        Converts the question-answer pair into a Python dictionary following the structure for QA pairs recommended in https://huggingface.co/course/chapter7/7?fw=pt  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, question, question_length, answer, answer_position_in_index):\n",
    "        \"\"\"\n",
    "        Constructs a question-answer pair having the passed parameters\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        question : str\n",
    "            question text\n",
    "        question_length : int\n",
    "            number of tokens of question text\n",
    "        answer : str\n",
    "            answer text\n",
    "        answer_position_in_index : int\n",
    "            position (index) of the first character of the answer text within original context\n",
    "        \"\"\"\n",
    "        self.id = uuid.uuid4().hex;\n",
    "        self.question = question\n",
    "        self.question_length = question_length\n",
    "        self.answer = answer\n",
    "        self.answer_start = answer_position_in_index\n",
    "  \n",
    "    def as_dict(self):\n",
    "        \"\"\"\n",
    "        Converts this question-answer pair into a Python dictionary following the structure for QA pairs recommended in https://huggingface.co/course/chapter7/7?fw=pt\n",
    "        {\n",
    "            'id': ....,\n",
    "            'question': ....,\n",
    "            'question_length': .....,\n",
    "            'answers':{\n",
    "                'text': [....],\n",
    "                'answer_start : [....]\n",
    "            }\n",
    "        }\n",
    "        Note that 'answers.text' and 'answers.answer_start' both are lists as in classical NL QA a question might have multiple answers. In our case, every question has exactly one answer, thus, one item per list.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Python dictionary\n",
    "        \"\"\"\n",
    "        a = {}\n",
    "        a[\"text\"] = [self.answer]\n",
    "        a[\"answer_start\"] = [self.answer_start]\n",
    "        q = {}\n",
    "        q[\"id\"] = self.id\n",
    "        q[\"question\"] = self.question\n",
    "        q[\"question_length\"] = self.question_length\n",
    "        q[\"answers\"] = a\n",
    "        return q\n",
    "\n",
    "class QuestionAnswerSample:\n",
    "    \"\"\"\n",
    "    Represents a question-answer sample consisting of a unique identifier, a title, a context, and multiple question-answer pairs extracted from the context \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    id : str\n",
    "        unique identifier of the sample\n",
    "    title : str\n",
    "        title of the sample\n",
    "    context : str\n",
    "        context (text)\n",
    "    questionAnswers : [QuestionAnswer]\n",
    "        question-answer pairs extracted from the context \n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    __str__():\n",
    "        Converts the sample including its question-answer pairs into a JSON structure following the structure for QA pairs recommended in https://huggingface.co/course/chapter7/7?fw=pt\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, context, questionAnswers: list, title = None):\n",
    "        \"\"\"\n",
    "        Constructs a question-answer sample having the passed parameters\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        context : str\n",
    "            context (text)\n",
    "        questionAnswers : [QuestionAnswer]\n",
    "            list of question-answer pairs\n",
    "        title : str\n",
    "            optional title of the sample\n",
    "        \"\"\"\n",
    "        self.id = uuid.uuid4().hex;\n",
    "        for qa in questionAnswers:\n",
    "            qa.id = self.id +\"_\"+qa.id\n",
    "\n",
    "        self.title = title;\n",
    "        self.context = context\n",
    "        self.questionAnswers = questionAnswers\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Converts the sample including its question-answer pairs into a JSON structure following the structure for QA pairs recommended in https://huggingface.co/course/chapter7/7?fw=pt\n",
    "        {\n",
    "            \"id\": ....,\n",
    "            \"title\": ....,\n",
    "            \"context\": ....,\n",
    "            \"questions\": [ //see QuestionAnswer.as_dict()]\n",
    "        }\n",
    "        \"\"\"\n",
    "        json_dict = {}\n",
    "        json_dict[\"id\"] = self.id\n",
    "        json_dict[\"title\"] = self.title\n",
    "        json_dict[\"context\"] = self.context\n",
    "        json_dict[\"questions\"] = [x.as_dict() for x in self.questionAnswers]\n",
    "        return json.dumps(json_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837aec8-5a23-4e00-8675-9e65c220ee70",
   "metadata": {},
   "source": [
    "## Methods for Creating Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290479c-62ef-441b-8eb3-61a932c5e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_path(operation_node: ApiInterfaceNode, p_http_verb_position = \"suffix\" , p_remove_path_parameter_indicator = False, p_remove_path_fragment_indicator = False):\n",
    "    path_segments = []\n",
    "    node = operation_node.parent\n",
    "    \n",
    "    if p_http_verb_position == \"suffix\":\n",
    "        path_segments.append(operation_node.key)\n",
    "    \n",
    "    while node is not None:\n",
    "        key = node.key.replace(\"/\",\"\").replace(\" \",\"-\").replace(\"!\",\"-\")\n",
    "        \n",
    "        if p_remove_path_parameter_indicator:\n",
    "            key = key.replace(\"{\",\"\").replace(\"}\",\"\")\n",
    "        if p_remove_path_fragment_indicator:\n",
    "            key = key.replace(\"#\",\".\")\n",
    "            \n",
    "        path_segment = key\n",
    "        \n",
    "        if path_segment:\n",
    "            path_segments.append(path_segment)\n",
    "        \n",
    "        if node.parent.node_type ==\"pathSegment\":\n",
    "            node = node.parent\n",
    "        else:\n",
    "            node = None\n",
    "    \n",
    "    if p_http_verb_position == \"prefix\":\n",
    "         path_segments.append(operation_node.key)\n",
    "    \n",
    "    path_segments.reverse()\n",
    "    return \".\".join(path_segments)\n",
    "        \n",
    "\n",
    "def filter_endpoints(endpoints: list, max_depth: int):\n",
    "    \"\"\"\n",
    "    Removes all endpoints from the passed list that exceed the specified maximum depth and returns the modified list.\n",
    "    Example: 'users.{userId}.get' has a depth of 3.\n",
    "    If 'max_depth' is None, the passed list will be returned without any removal.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    endpoints : [str]\n",
    "        List of endpoints (strings)\n",
    "    max_depth : int\n",
    "        maximum depth (can be None)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Modified list of endpoints\n",
    "    \"\"\"\n",
    "    filtered_endpoints = []\n",
    "    for endpoint in endpoints:\n",
    "        if max_depth == None or len(endpoint.split(\".\")) <= max_depth:\n",
    "            filtered_endpoints.append(endpoint)\n",
    "    return filtered_endpoints\n",
    "\n",
    "\n",
    "def build_context_string(endpoints: list, p_sort_by_name: bool = False, p_shuffle: bool = False):\n",
    "    \"\"\"\n",
    "    Removes duplicate endpoints from the passed endpoint list, optionally sort (ascending order) or shuffle the list and finally concatenates the remaining endpoint items to a string with spaces as speparator between items.\n",
    "    The method returns this resulting string. The returned string is empty, i.e., '', if the list of endpoints is empty.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    endpoints : [string]\n",
    "        list of endpoints\n",
    "    p_sort_by_name : bool\n",
    "        if set to True (default value is False), the method will sort the list of endpoint items in ascending order\n",
    "    p_shuffle : bool\n",
    "        if set to True (default value is False), the method will shuffle the list of endpoint items\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    String containing the concatenated endpoint items\n",
    "    \"\"\"\n",
    "    \n",
    "    if p_shuffle:\n",
    "        assert not p_sort_by_name, \"Cannot shuffle and sort endpoints at the same time\"\n",
    "    if p_sort_by_name:\n",
    "        assert not p_shuffle, \"Cannot shuffle and sort endpoints at the same time\"\n",
    "    \n",
    "    endpoints_without_duplicate_items = []\n",
    "    deduplication_set = set()\n",
    "    \n",
    "    for endpoint in endpoints:\n",
    "        if not endpoint in deduplication_set:\n",
    "            endpoints_without_duplicate_items.append(endpoint)\n",
    "            deduplication_set.add(endpoint)\n",
    "            \n",
    "    # shuffle (if enabled)\n",
    "    if p_shuffle:\n",
    "        random.shuffle(endpoints_without_duplicate_items)\n",
    "        \n",
    "    # sort by name (if enabled)\n",
    "    if p_sort_by_name:\n",
    "        endpoints_without_duplicate_items.sort()\n",
    "    \n",
    "    return \" \".join(endpoints_without_duplicate_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f35ef3-11e6-4508-8738-fbe5005f22c9",
   "metadata": {},
   "source": [
    "## Methods for Creating Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b915c-50d2-4dc2-a4f3-86559855eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_inline_uris(text: str):\n",
    "    \"\"\"\n",
    "    Removes URIs from the passed string and returns the modified string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        input string that should be scanned for URIs\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Modified string\n",
    "    \"\"\"\n",
    "    tokens = text.split(' ')\n",
    "    for token in tokens:\n",
    "        if \"http:\" in token.lower() or \"https:\" in token.lower():\n",
    "            text = text.replace(token,\" \")\n",
    "    return text\n",
    "\n",
    "def truncate_question(question: str, max_question_length: int):\n",
    "    \"\"\"\n",
    "    Truncates the passed question (string) if the number of tokens exceeds the passed maximum question length. If the passed question is too long, the method tries to split it into sentences and concatenates these sentences\n",
    "    until maximum question length is exceeded. The method returns the truncated question, its length (number of tokens), and the whether it has been  truncated (True) or not (False)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "        Original question that should be truncated\n",
    "    max_question_length : int\n",
    "        Maximum number of tokens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The truncated question, its length, and whether it has been truncated (True) or not (False)\n",
    "    \n",
    "    \"\"\"\n",
    "    # determine length of original question\n",
    "    length = get_length(question)\n",
    "    \n",
    "    # if question length (number of tokens determined by tokenizer) does not exceed max. question length\n",
    "    if length <= max_question_length:\n",
    "        # return question without any modifications\n",
    "        return question, length, False\n",
    "    \n",
    "    # if question length exceeds max. question length\n",
    "    else:\n",
    "        # (Try to) split question into sentences\n",
    "        sentences = question.split(\".\")\n",
    "        \n",
    "        # calculate length for each sentende \n",
    "        sentences_and_lengths = [(x,get_length(x)) for x in sentences] # generates a tuple of (sentence,length) for each sentence\n",
    "        \n",
    "        # reset question and length\n",
    "        question = \"\"\n",
    "        length = 0\n",
    "        \n",
    "        # concatenate sentences until max. question length is reached\n",
    "        for sentence in sentences_and_lengths:\n",
    "            sentence_text = sentence[0]\n",
    "            sentence_length = sentence[1]\n",
    "            \n",
    "            # if sentence is not empty and new total length does not exceed max. question length\n",
    "            if sentence_text is not None and length + sentence_length < max_question_length:\n",
    "                question = question + sentence_text+\".\"\n",
    "                length = length + sentence_length + 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        return question, get_length(question), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e4a84-4b10-4448-8475-37517eb38c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test truncate_question\n",
    "question = \"This is a short example. We want to test, whether truncate_question works as expected. Hello World\"\n",
    "truncate_question(question,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f4a28-08b3-40b0-8a9c-3f561bb6a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_length(\"This is a short\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e2a2c-5afb-40d8-a25c-71255f6b3c9d",
   "metadata": {},
   "source": [
    "## Methods for Creating Question-Answer Pairs and Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b31e2-f12a-4d2e-b690-f76517d80c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_start(context: str,answer_text: str):\n",
    "    \"\"\"\n",
    "    Returns the position of the first character of the passed answer text in the specified context or None if the answer is not in the context.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    context : str\n",
    "        context\n",
    "    answer_text:\n",
    "        answer text\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Position of the first character of the answer text or None if the answer is not in the context\n",
    "    \"\"\"\n",
    "    \n",
    "    position = 0\n",
    "    for property in context.split():\n",
    "        if answer_text == property:\n",
    "            return position\n",
    "        else:\n",
    "            position += len(property)\n",
    "        position+=1 # for each space\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed336e23-f372-4cd5-9f19-11602bae5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test get_answer_start\n",
    "context = \"user.address user.address.street user.address.city user\"\n",
    "get_answer_start(context,\"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac936c3-e141-4cd6-baac-007f064c8992",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,char in enumerate(context):\n",
    "    print(i,\" \",char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ed41e-0679-4d7c-b270-723a0d33ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question_answer_pair(operation_node: ApiInterfaceNode, context: str, path: str, p_min_question_length: int = None , p_max_question_length: int = None, p_remove_uris: bool = False, p_max_depth: int = None):\n",
    "    \"\"\"\n",
    "    Create and returns a Question-Answer pair for the passed operation node.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : ApiInterfaceNode\n",
    "        the operation node\n",
    "    context : str\n",
    "        context, required for calculating the position (character-based index) of the answer\n",
    "    path : str\n",
    "        the endpoint path of the operation\n",
    "    p_min_question_length : int\n",
    "        Optional parameter (default is None) that specifies the minimum length (number of tokens) that a question must have. If the parameter is None, the minimum length is one token\n",
    "    p_max_question_length : int\n",
    "        Optional parameter (default is None) that specifies the maximum length (number of tokens) that a question may have. If the parameter is None, the maximum length is unlimited\n",
    "    p_remove_uris : bool\n",
    "        Optional parameter (default is False) indicating whether URIs should be removed from questions\n",
    "    p_max_depth : int\n",
    "        Optional parameter (default is None) that specifies the maximum depth that an answer (i.e. the endpoint) may have. If the parameter is None, there is no depth limitation\n",
    "    \n",
    "    Results\n",
    "    -------\n",
    "    Created Question-Answer pair\n",
    "    \"\"\"\n",
    "    global cnt_operations\n",
    "    cnt_operations+=1\n",
    "    \n",
    "    valid = True\n",
    "    \n",
    "    if get_length(operation_node.method_description) >= get_length(operation_node.method_summary):\n",
    "        description = operation_node.method_description\n",
    "    else:\n",
    "        description = operation_node.method_summary\n",
    "    \n",
    "    # check whether description is empty\n",
    "    if not description:\n",
    "        global cnt_operations_without_descriptions\n",
    "        cnt_operations_without_descriptions += 1 \n",
    "    \n",
    "    # check whether description consists of at least X tokens (only if min. question length is enabled and description is not empty):\n",
    "    if p_min_question_length and description: \n",
    "        length = get_length(description)\n",
    "        if length < p_min_question_length:\n",
    "            global cnt_operations_with_too_short_descriptions\n",
    "            cnt_operations_with_too_short_descriptions+=1\n",
    "            # set description to none in order to skip the following steps\n",
    "            description = None\n",
    "    \n",
    "        \n",
    "    # continue only if there is a description\n",
    "    if description:\n",
    "    \n",
    "        # Step 1 (optional): remove inline URIs:\n",
    "        if p_remove_uris:\n",
    "            description = remove_inline_uris(description)\n",
    "                \n",
    "        # Step 2: remove unecessary whitespaces\n",
    "        while '  ' in description:\n",
    "            description = description.replace('  ',' ')\n",
    "            \n",
    "        # Step 3 (optional): truncate description; this step ensures that the length of the returned description does not exceed the maximum length by truncating it.\n",
    "        # Therefore truncate_question might return an empty question, i.e., question with len = 0, if the question cannot be truncated\n",
    "        if p_max_question_length is not None:\n",
    "            description, length, truncated = truncate_question(description, p_max_question_length)\n",
    "            if truncated:\n",
    "                global cnt_operations_with_truncated_descriptions\n",
    "                cnt_operations_with_truncated_descriptions += 1\n",
    "        else:\n",
    "            length = get_length(description)\n",
    "            \n",
    "        # Step 4: again, check whether description contains enough tokens after truncation\n",
    "        if p_min_question_length:\n",
    "            min_len = p_min_question_length\n",
    "        else:\n",
    "            min_len = 1\n",
    "\n",
    "        if length < min_len:\n",
    "            # The description could not be truncated means that:\n",
    "            # 1.) The description is still too long, even after removing trailing sentences (i.e., truncate_question returns an empty description)\n",
    "            # 2.) The description could be truncated, but the resultung description does not contain enough tokens\n",
    "            global cnt_operations_with_descriptions_that_could_not_be_truncated \n",
    "            cnt_operations_with_descriptions_that_could_not_be_truncated += 1\n",
    "            valid = False\n",
    "    else:\n",
    "        valid = False\n",
    "        \n",
    "    # count overall issues with description\n",
    "    if not valid:\n",
    "        global cnt_operation_with_description_constraint_violation\n",
    "        cnt_operation_with_description_constraint_violation+=1\n",
    "        \n",
    "    # check whether endpoint path exceeds max. depth\n",
    "    if p_max_depth is not None and len(path.split(\".\")) > p_max_depth:\n",
    "        global cnt_operations_with_too_deep_path \n",
    "        cnt_operations_with_too_deep_path += 1\n",
    "        valid = False\n",
    "    \n",
    "    # if all constraints are satisfied \n",
    "    if valid:\n",
    "            \n",
    "        # Build answer:\n",
    "        answer = path\n",
    "            \n",
    "        # Calculate answer position in context\n",
    "        answer_start = get_answer_start(context,answer)\n",
    "        assert answer_start is not None, \"Answer '\"+answer+\"' of question generated from ID '\"+operation_node.id+\"' is not in context\"\n",
    "\n",
    "        # Create Question-Answer pair and add it to list\n",
    "        question_answer = QuestionAnswer(description,length,answer,answer_start)\n",
    "        return question_answer\n",
    "    else:\n",
    "        global cnt_invalid_operations\n",
    "        cnt_invalid_operations+=1\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e1a399-fcd9-4aea-9e60-35d525162259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question_answer_samples_for_api(\n",
    "    api_node: ApiInterfaceNode, \n",
    "    p_min_question_length: int = None, \n",
    "    p_max_question_length: int = None, \n",
    "    p_remove_uris: bool = False, \n",
    "    p_max_depth: int = None, \n",
    "    p_http_verb_position = \"suffix\" , \n",
    "    p_remove_path_parameter_indicator = False, \n",
    "    p_remove_path_fragment_indicator = False,  \n",
    "    p_max_questions_per_sample: int = None, \n",
    "    p_sort_by_name: bool = False, \n",
    "    p_shuffle_context: bool = False):\n",
    "    \"\"\"\n",
    "    Creates and returns one or multiple Question-answer samples for the passed API node. The decision whether one or multiple samples are created depends on the 'p_max_questions_per_sample' threshold as well as the size (number of endpoints) of the API.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    api_node : ApiInterfaceNode\n",
    "        API node\n",
    "    p_max_questions_per_sample : int\n",
    "        Optional parameter (default is None) that specifies the maximum number of question-answer pairs in one question-answer sample. There is no upper limit, if the parameter is None \n",
    "    p_min_question_length : int\n",
    "        Optional parameter (default is None) that specifies the minimum length (number of tokens) that a question must have. If the parameter is None, the minimum length is one token\n",
    "    p_max_question_length : int\n",
    "        Optional parameter (default is None) that specifies the maximum length (number of tokens) that a question may have. If the parameter is None, the maximum length is unlimited\n",
    "    p_remove_uris : bool\n",
    "        Optional parameter (default is False) indicating whether URIs should be removed from questions\n",
    "    p_max_depth : int\n",
    "        Optional parameter (default is None) that specifies the maximum depth that an XPath (as answer as well as in context) may have. If the parameter is None, there is no depth limitation\n",
    "    p_http_verb_position : str\n",
    "        Optional parameter (default is \"suffix\") that specifies the position of the HTTP verb in the endpoint path. Allowed values are \"prefix\" and \"suffix\".\n",
    "    p_remove_path_parameter_indicator : bool\n",
    "        Optional parameter (default is False) that specifies whether path parameter indicators should be removed from path segments\n",
    "    p_remove_path_fragment_indicator : bool\n",
    "        Optional parameter (default is False) that specified whether fragment indicators should be removed from path segments \n",
    "    p_max_questions_per_sample : int\n",
    "        Optional parameter (default is None) that specifies the maximum number of Question-Answer pairs per sample. The method distributes the Question-Answer pairs to mulitple samples if this limit is exceeded.\n",
    "    p_sort_by_name : bool\n",
    "        if set to True (default value is False), the method will sort XPaths in context in ascending order\n",
    "    p_shuffle : bool\n",
    "        if set to True (default value is False), the method will shuffle XPaths in context\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List of created Question-Answer samples (even if one sample is created, it is a list)\n",
    "    \"\"\"\n",
    "    \n",
    "    operation_nodes = extract_nodes(api_node, node_type=\"method\")\n",
    "    \n",
    "    question_answer_pairs = []\n",
    "    \n",
    "    # build context\n",
    "    endpoints = []\n",
    "    for operation_node in operation_nodes:\n",
    "        endpoints.append(build_path(operation_node, p_http_verb_position, p_remove_path_parameter_indicator, p_remove_path_fragment_indicator))\n",
    "    if p_max_depth:\n",
    "        endpoints = filter_endpoints(endpoints,p_max_depth)\n",
    "    context = build_context_string(endpoints,p_sort_by_name,p_shuffle_context)\n",
    "    \n",
    "    # create question-answer pair for each operation\n",
    "    for operation_node in operation_nodes:\n",
    "        # build endpoint path\n",
    "        path = build_path(operation_node, p_http_verb_position, p_remove_path_parameter_indicator, p_remove_path_fragment_indicator)\n",
    "        # create question-answer pair\n",
    "        question_answer_pair = create_question_answer_pair(operation_node, context, path, min_question_length, max_question_length, remove_uris, max_depth)\n",
    "        if question_answer_pair:\n",
    "            question_answer_pairs.append(question_answer_pair)\n",
    "        \n",
    "    # check if at least one question-answer pair has been created:\n",
    "    if question_answer_pairs:\n",
    "        if p_max_questions_per_sample:\n",
    "            samples = []\n",
    "            \n",
    "            while question_answer_pairs:\n",
    "                counter = 0\n",
    "                partial_question_answer_pairs = []\n",
    "                while len(question_answer_pairs) > 0 and counter < p_max_questions_per_sample:\n",
    "                    partial_question_answer_pairs.append(question_answer_pairs.pop())\n",
    "                    counter += 1\n",
    "                sample = QuestionAnswerSample(context, partial_question_answer_pairs, api_node.id)\n",
    "                samples.append(sample)\n",
    "            \n",
    "            if len(samples) > 1:\n",
    "                global cnt_split_samples\n",
    "                cnt_split_samples += 1\n",
    "            return samples\n",
    "        else:\n",
    "            sample = QuestionAnswerSample(context, question_answer_pairs, api_node.id)\n",
    "            return [sample]\n",
    "    else:\n",
    "        global cnt_apis_without_samples\n",
    "        cnt_apis_without_samples+=1\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee6602-f062-47ba-9f38-979b98896fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_cnt = 0\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "for api in tqdm(apis):\n",
    "    if int(api.api_key) in excluded_api_keys:\n",
    "        print(\"Skip \",api.api_name,\" (\",api.api_key,\")\")\n",
    "        continue\n",
    "\n",
    "    question_cnt_per_api = 0\n",
    "    samples = []\n",
    "\n",
    "    for i in range(original_retakes):\n",
    "        samples_of_api = create_question_answer_samples_for_api(\n",
    "            api,\n",
    "            min_question_length,\n",
    "            max_question_length,\n",
    "            remove_uris,\n",
    "            max_depth,\n",
    "            http_verb_position, \n",
    "            remove_path_parameter_indicator, \n",
    "            remove_path_fragment_indicator,\n",
    "            max_questions_per_sample,\n",
    "            sort_by_name,\n",
    "            False)\n",
    "        if samples_of_api:\n",
    "            samples += samples_of_api\n",
    "    \n",
    "    for i in range(shuffled_retakes):\n",
    "        samples_of_api = create_question_answer_samples_for_api(\n",
    "            api,\n",
    "            min_question_length,\n",
    "            max_question_length,\n",
    "            remove_uris,\n",
    "            max_depth,\n",
    "            http_verb_position, \n",
    "            remove_path_parameter_indicator, \n",
    "            remove_path_fragment_indicator,\n",
    "            max_questions_per_sample,\n",
    "            sort_by_name,\n",
    "            True)\n",
    "        if samples_of_api:\n",
    "            samples += samples_of_api\n",
    "\n",
    "    if samples:\n",
    "        for sample in samples:\n",
    "            question_cnt_per_api += len(sample.questionAnswers)\n",
    "            question_cnt += len(sample.questionAnswers)\n",
    "        results.append({\n",
    "            \"samples\":samples,\n",
    "            \"api_key\":api.api_key,\n",
    "            \"api_name\":api.api_name,\n",
    "            \"api_version_key\":api.api_version_key,\n",
    "            \"api_version_name\":api.api_version_name,\n",
    "            \"question_cnt_per_api\":question_cnt_per_api\n",
    "        })      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7273a9-841d-4714-bf09-39a2e0543c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = sorted(results, key=lambda item: item[\"question_cnt_per_api\"],reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a8bd49-38ce-4da1-b360-4df3c9456371",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [[] for i in range(number_of_chunks)]\n",
    "\n",
    "with open(output_path+datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")+\".log.csv\",\"w\") as log_file:\n",
    "    log_file.write(\"API Key;API Name;API Version Key; API Version;#Samples;#Questions;Out File\\n\")\n",
    "    for result in sorted_results:\n",
    "        smallest_chunk_index = 0\n",
    "        smallest_chunk_size = None\n",
    "        for i in range(number_of_chunks):\n",
    "            num_questions = 0\n",
    "            for sample in chunks[i]:\n",
    "                num_questions+=result[\"question_cnt_per_api\"]\n",
    "            if smallest_chunk_size == None or num_questions < smallest_chunk_size:\n",
    "                smallest_chunk_size = num_questions\n",
    "                smallest_chunk_index = i\n",
    "        chunks[smallest_chunk_index]+= result[\"samples\"]\n",
    "        filename = str(smallest_chunk_index)+\".json\"\n",
    "\n",
    "        log_file.write(str(result[\"api_key\"])+\";\"+str(result[\"api_name\"])+\";\"+str(result[\"api_version_key\"])+\";\"+str(result[\"api_version_name\"])+\";\"+str(len(result[\"samples\"]))+\";\"+str(result[\"question_cnt_per_api\"])+\";\"+filename+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6bb44-8bc7-4742-a629-aa1f635851c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of processed operations\n",
    "print(\"# Parameters: \",cnt_operations)\n",
    "\n",
    "# number of operations that do not satisfy the contraints required to generate a question-answer pair\n",
    "print(\"# Invalid Parameters: \",cnt_invalid_operations)\n",
    "\n",
    "# number of generated question-answer pairs, which is equal to the number of paramters that satisfy all contraints for being a question-answer pair\n",
    "print(\"# Questions: \",question_cnt)\n",
    "assert question_cnt == cnt_operations-cnt_invalid_operations, \"Mismatch between number of generated question-answer pairs and invalid parameters\"\n",
    "\n",
    "# number of APIs from which no question-answer pairs can be generated, since the API does not contain any endpoint that satisfies the constraints\n",
    "print(\"# API without any QA samples: \", cnt_apis_without_samples)\n",
    "\n",
    "# (added in v3.2) number of operations whose descriptions do not satisfy at least one constraint\n",
    "print(\"# Operations with description violation: \", cnt_operation_with_description_constraint_violation)\n",
    "# number of operations that must be excluded due to missing descriptions\n",
    "print(\"# Operations without descriptions: \", cnt_operations_without_descriptions)\n",
    "# number of operations whose path exceeds the maximum depth\n",
    "print(\"# Operations with too deep XPaths: \", cnt_operations_with_too_deep_path)\n",
    "# number of operations whose description are too short\n",
    "print(\"# Operations with too short descriptions (even before truncation): \", cnt_operations_with_too_short_descriptions)\n",
    "\n",
    "# number of operations whose description cannot be truncated, since:\n",
    "# 1.) The description is still too long, even after removing trailing sentences (i.e., truncate_question returns an empty description)\n",
    "# 2.) The description could be truncated, but the resultung description does not contain enough tokens\n",
    "print(\"# Operations with descriptions could not be truncated (because they were too long or too short after truncation): \", cnt_operations_with_descriptions_that_could_not_be_truncated)\n",
    "\n",
    "assert cnt_invalid_operations <= cnt_operations_without_descriptions+cnt_operations_with_too_deep_path+cnt_operations_with_too_short_descriptions+cnt_operations_with_descriptions_that_could_not_be_truncated, \"Mismatch between number of invalid operations and numbers specifying reasons for being invalid\"\n",
    "\n",
    "# operations with descriptions that are truncated\n",
    "print(\"# Operations with descriptions that could be truncated: \", cnt_operations_with_truncated_descriptions)\n",
    "\n",
    "\n",
    "print(\"# Original samples that have been split into multiple samples: \", cnt_split_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05159794-7b3b-4833-84de-5de9e748b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle samples\n",
    "for i in range(number_of_chunks):\n",
    "    random.shuffle(chunks[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e254d1b-cd01-4ca2-ba8a-21385a1abbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print chunks\n",
    "for i in range(number_of_chunks):\n",
    "    q_cnt = 0\n",
    "    for sample in chunks[i]:\n",
    "            q_cnt += len(sample.questionAnswers)\n",
    "    print(i,\": \",len(chunks[i]),\" samples / \",q_cnt,\" questions (\", (q_cnt/question_cnt)*100,\"%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b597e-a433-404b-8c43-c7ae0f7e273f",
   "metadata": {},
   "source": [
    "# Analyze Question and Paragraph Length (added in v3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651da46-5c45-46a6-8563-6de2dab1b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_word_len = []\n",
    "question_token_len = []\n",
    "parameter_xpath_len = []\n",
    "parameters_per_schema = []\n",
    "schema_token_len = []\n",
    "\n",
    "unique_schema_set = set()\n",
    "\n",
    "for i in range(number_of_chunks):\n",
    "    for sample in tqdm(chunks[i]):\n",
    "        token_s_len = get_length(sample.context)\n",
    "        for question in sample.questionAnswers:\n",
    "            word_q_len = len(word_tokenize(question.question))\n",
    "            token_q_len = get_length(question.question)\n",
    "            \n",
    "            #if word_q_len == 117:\n",
    "            #    print(question.question)\n",
    "            \n",
    "            question_word_len.append(word_q_len)\n",
    "            question_token_len.append(token_q_len)\n",
    "            parameters_per_schema.append(len(sample.context.split(\" \")))\n",
    "            schema_token_len.append(token_s_len)\n",
    "            \n",
    "            if sample.context not in unique_schema_set:\n",
    "                 unique_schema_set.add(sample.context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b198d8-739d-4b46-99a9-d35fda0f0c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean|median|stdev|min|max for question token length: \", statistics.mean(question_token_len),\"|\",statistics.median(question_token_len),\"|\",statistics.stdev(question_token_len),\"|\",min(question_token_len),\"|\",max(question_token_len))\n",
    "print(\"Mean|median|stdev|min|max for question word length: \", statistics.mean(question_word_len),\"|\",statistics.median(question_word_len),\"|\",statistics.stdev(question_word_len),\"|\",min(question_word_len),\"|\",max(question_word_len))\n",
    "print(\"Mean|median|stdev|min|max for number of parameters per schema: \", statistics.mean(parameters_per_schema),\"|\",statistics.median(parameters_per_schema),\"|\",statistics.stdev(parameters_per_schema),\"|\",min(parameters_per_schema),\"|\",max(parameters_per_schema))\n",
    "print(\"Mean|median|stdev|min|max for schema token length: \", statistics.mean(schema_token_len),\"|\",statistics.median(schema_token_len),\"|\",statistics.stdev(schema_token_len),\"|\",min(schema_token_len),\"|\",max(schema_token_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f454015f-902b-467f-88de-8e63e0fecb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(parameters_per_schema, color = 'blue', edgecolor = 'black',\n",
    "         bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31496e-66a3-443f-8f45-3fed20998a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_context_token_len = []\n",
    "parameters_per_unique_context = []\n",
    "\n",
    "for context in tqdm(unique_schema_set):\n",
    "    context_len = get_length(context)\n",
    "    unique_context_token_len.append(context_len)\n",
    "    parameters_per_unique_context.append(len(context.split(\" \")))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df76e0-ea8a-4b1e-bc25-9b10ff340310",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique schemas: \",len(unique_schema_set))\n",
    "print(sum(parameters_per_unique_context))\n",
    "print(\"Mean|median|stdev|min|max for schema token length: \", statistics.mean(unique_context_token_len),\"|\",statistics.median(unique_context_token_len),\"|\",statistics.stdev(unique_context_token_len),\"|\",min(unique_context_token_len),\"|\",max(unique_context_token_len))\n",
    "print(\"Mean|median|stdev|min|max for number of parameters per schema: \", statistics.mean(parameters_per_unique_context),\"|\",statistics.median(parameters_per_unique_context),\"|\",statistics.stdev(parameters_per_unique_context),\"|\",min(parameters_per_unique_context),\"|\",max(parameters_per_unique_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ee6a2-d38b-4579-a8b3-6944b3a6d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(parameters_per_unique_context, color = 'blue', edgecolor = 'black',\n",
    "         bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1772ede-6b4f-4226-82a7-f461b9bdef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write chunks\n",
    "for i in range(number_of_chunks):\n",
    "  with open(output_path+str(i)+\".json\",\"w\") as file:\n",
    "    for sample in chunks[i]:\n",
    "      file.write(str(sample))\n",
    "      file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1afabe-6402-4991-8acb-f207ab8000f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write validation set\n",
    "validation_index = 2\n",
    "with open(output_path+\"validation.json\",\"w\") as file:\n",
    "    for sample in chunks[validation_index]:\n",
    "        file.write(str(sample))\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743001a-8ddc-442e-b4c2-5af4dfc6a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write test set\n",
    "test_index = 9\n",
    "with open(output_path+\"test.json\",\"w\") as file:\n",
    "    for sample in chunks[test_index]:\n",
    "        file.write(str(sample))\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194b81b-942b-47a7-8f5c-a0090ea017ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = [0,1,3,4,5,6,7,8]\n",
    "train_samples = []\n",
    "for i in train_indices:\n",
    "    train_samples += chunks[i]\n",
    "random.shuffle(train_samples)\n",
    "\n",
    "with open(output_path+\"train.json\",\"w\") as file:\n",
    "    for sample in train_samples:\n",
    "        file.write(str(sample))\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca77ed7-bfe8-4ca2-bbd9-a7c6eecc3d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hf]",
   "language": "python",
   "name": "conda-env-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
