{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0123ab4e-10b9-455e-aae8-1aae713597f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes v4:\n",
    "# - Bugfixing in tokenize_samples: If sample has a single token answer and that token is the last token in context, but not the last token of the sample (i.e. there are further <pad> tokens), then token_start_index is greater than token_end_index\n",
    "# - Revision of code in tokenize_samples: Outsourcing of code fragments into separate functions to reduce complexity and redundancy of code\n",
    "\n",
    "# Minor changes v4.1:\n",
    "# - Print statement in preparation for training has been modified: print(\"#Samples with answer:\",total_samples-samples_with_null_answer) instead of print(\"#Samples with answer:\",samples_with_null_answer)\n",
    "\n",
    "# Minor changes v4.2:\n",
    "# - Final network update (training), if there are batch samples left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88304467-46b3-43b8-a532-70f6fc8ef4a1",
   "metadata": {},
   "source": [
    "# Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae6f9c-a6c1-463e-973a-744ec10cabb9",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc6bcf-46de-4a95-8447-143229515893",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers datasets huggingface_hub\n",
    "! pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4ac87d-c136-4b41-8a13-ca584c992562",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27dcccd-342e-4bd6-888a-c693bba2bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading the input dataset from disk\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering, create_optimizer\n",
    "\n",
    "# we use Weights & Biases \n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# ... as well as TensorFlow for training\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6207117d-1c48-4401-81b0-8b981c17de75",
   "metadata": {},
   "source": [
    "## Check GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263dc1e3-1906-4e5a-8852-d17d4a4be433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TensorFlow version and whether GPU is available (there should be at least on physical device being listed)\n",
    "tf.__version__, tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0871a31-fa3c-4865-938e-49a7eef89537",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92ab9b7-e22a-4e28-a5fa-b0794b674e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project and run name on Weights & Biases\n",
    "project_name = \"WANDB_PROJECT_NAME\"\n",
    "run_name = \"WANDB_RUN_NAME\"\n",
    "\n",
    "# path to directory containing QA-samples created in 'Data Preparation V3'\n",
    "input_path = \"/home/user/output_directory_of_data_preparation/\"\n",
    "\n",
    "# path to directory where checkpoints should be stored\n",
    "checkpoint_path= \"/home/user/directory_for_checkpoints/\"\n",
    "\n",
    "# path to directory used for caching\n",
    "caching_path = \"/home/user/directory_for_hf_cache/\"\n",
    "\n",
    "# Checkpoint of base model on Hugging Face\n",
    "model_checkpoint = \"microsoft/codebert-base\"\n",
    "\n",
    "# Parameters for tokenizing:\n",
    "max_length = 512  # Maximum number of features (i.e. indices) consisting of tokenized question and context in a tokenized sample\n",
    "doc_stride = 128  # Allowed overlap of the tokenized context if a QA-sample must be split into multiple tokenized samples due to length limitations\n",
    "\n",
    "# As a QA-sample might be split into multiple tokenized samples resulting into the situation that only one or a few tokenized samples contain the answer in its sub-context and the rest are no answerable samples,\n",
    "# we limit the number of no answerable samples by the following threshold parameter:\n",
    "max_no_answers_per_possible_answer = 3 # for each possible answer pick X no answerable samples\n",
    "\n",
    "# Training parameters:\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 10\n",
    "weight_decay = 0.01  # never be used?\n",
    "xla = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76780406-0cec-4290-9476-3ba02048193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for randomly picking X no answerable samples from all tokenized samples of a QA-sample\n",
    "random.seed(42)\n",
    "chosen_sample_indices = dict()\n",
    "first_preprocessing_iteration = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce97d1-baca-4489-9cfc-32b673468a4e",
   "metadata": {},
   "source": [
    "# Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00432748-174d-448e-a1a4-d59c7af1a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "pad_on_right = tokenizer.padding_side == \"right\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e48014f-cc3c-42c0-9b27-859d6da9ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_samples(examples):\n",
    "    \"\"\"\n",
    "    'Data Preparation V3' has created a dataset where multiple question-answer pairs, each sharing same context, are collated within one sample. \n",
    "    This function expands the dataset structure so that in the resulting dataset each row represents a single question-answer pair.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : dict()\n",
    "        Dictionary having the format:\n",
    "        {\n",
    "            'id': [string],\n",
    "            'title': [string],\n",
    "            'context': [string],\n",
    "            'questions':\n",
    "                [\n",
    "                    {\n",
    "                        'id': string\n",
    "                        'question': string\n",
    "                        'question_length': int\n",
    "                        'answers': \n",
    "                            {\n",
    "                                'text': [string],\n",
    "                                'answer_start': [int]\n",
    "                            }\n",
    "                    }\n",
    "                ]\n",
    "   \n",
    "        }\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Python dictionary having the format:\n",
    "    {\n",
    "        'id': [string],\n",
    "        'title': [string],\n",
    "        'context': [string],\n",
    "        'question': [string],\n",
    "        'question_id': [string],\n",
    "        'answers': \n",
    "            [\n",
    "                {\n",
    "                    'text': [string],\n",
    "                    'answer_start': [int]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "    }\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    d[\"id\"] = []\n",
    "    d[\"title\"] = []\n",
    "    d[\"context\"] = []\n",
    "    d[\"question\"] = []\n",
    "    d[\"question_id\"] = []\n",
    "    d[\"answers\"] = []\n",
    "  \n",
    "    for i in range(len(examples[\"id\"])):\n",
    "        for question in examples[\"questions\"][i]:\n",
    "            d[\"id\"].append(examples[\"id\"][i])\n",
    "            d[\"title\"].append(examples[\"title\"][i])\n",
    "            d[\"context\"].append(examples[\"context\"][i])\n",
    "            d[\"question\"].append(question[\"question\"])\n",
    "            d[\"question_id\"].append(question[\"id\"]) \n",
    "            d[\"answers\"].append(question[\"answers\"])\n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7fe12e2-3ff4-4afe-b3e1-85be5adae3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_offset_mapping(sequence_ids, context_index, offset_mapping):\n",
    "    \"\"\"\n",
    "    Masks the passed 'offset_mapping', i.e. replaces all its entries with 'None' that represent tokens that are not part of the context.\n",
    "    The method returns the masked offset mapping.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence_ids :\n",
    "        Vector that defines for each token whether the token is part of the question, of the context, or is a special token.\n",
    "        An entry representing a token that is part of the context must have the value specified in 'context_index'\n",
    "    context_index : int\n",
    "        Value that is used to mark a token as part of the context in 'sequence_ids'\n",
    "    offset_mapping\n",
    "        Vector that contains for each token its start and end index on character level in the original input (consisting of question, context, and special characters). \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Masked 'offset_mapping'; entries that represent tokens that are not part of the context have the value 'None'\n",
    "    \"\"\"\n",
    "    masked_offset_mapping = [(o if sequence_ids[k] == context_index else None) for k, o in enumerate(offset_mapping)]\n",
    "    return masked_offset_mapping\n",
    "\n",
    "def calc_start_end_index(cls_index, offset_mapping, answers):\n",
    "    \"\"\"\n",
    "    Calculates start and end index on token level of the passed answer based on the specified 'offset_mapping'.\n",
    "    The method returns start and end index (token level) as well as flags indicating whether answer is out of span (third return parameter) and\n",
    "    whether the sample is erroneous since the calculated start index is greater than end index (fourth return parameter). If the answer is out of span,\n",
    "    start and end index have the value of the passed 'cls_index'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cls_index : int\n",
    "        Index of the [CLS] token that is used as start and end index if the answer is out of span\n",
    "    offset_mapping \n",
    "        Vector that contains for each token its start and end index on character level in the original input (consisting of question, context, and special characters). \n",
    "        All entries that represent a token that is not part of the context must be 'None' (use the method 'mask_offset_mapping(...)' to mask the 'offset_mapping' vector before using this method).\n",
    "    answers: dict()\n",
    "        Dictionary having the format:\n",
    "            {\n",
    "                'text': [string],\n",
    "                'answer_start': [int]\n",
    "            }\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    start_index : int\n",
    "        Calculated start index on token level (might have the value of 'cls_index' if the answer is out of span)\n",
    "    end_index : int\n",
    "        Calculated end index on token leven (might have the value of 'cls_index' if the answer is out of span)\n",
    "    answer_out_of_span : bool\n",
    "        Flag that indicates whether the answer is completely or partially out of span (True) or within span (False)\n",
    "    erroneous : bool\n",
    "        Flag that indicates whether the result of the calculation is erroneous. This might be the case if the answer \n",
    "        does not start or ends with the first/last character of a token, but elsewhere within the span of a token. \n",
    "    \"\"\"\n",
    "    \n",
    "    # if no answer is given\n",
    "    if not answers[\"answer_start\"]:\n",
    "        # return 'answer out of span' result\n",
    "        return cls_index, cls_index, True, False\n",
    "    \n",
    "    # load answer start (on character level)\n",
    "    start_char = answers[\"answer_start\"][0]\n",
    "    \n",
    "    # calculate answer end (on character level)\n",
    "    end_char = start_char + len(answers[\"text\"][0])\n",
    "    \n",
    "    # initialize start and end index (on token level)\n",
    "    start_token = 0\n",
    "    end_token = len(offset_mapping) - 1\n",
    "    \n",
    "    # move start index (token level) to the first token of the context\n",
    "    while offset_mapping[start_token] is None:\n",
    "        start_token += 1\n",
    "    \n",
    "    # move end index (token level) to the last token of the context\n",
    "    while offset_mapping[end_token] is None:\n",
    "        end_token -= 1\n",
    "        \n",
    "    # check whether answer is within context span:\n",
    "    # if start_char is smaller than index of first character of first context token OR end_char is greater than index of last character of last context token\n",
    "    if start_char < offset_mapping[start_token][0] or end_char > offset_mapping[end_token][1]:\n",
    "        # return 'answer out of span' result\n",
    "        return cls_index, cls_index, True, False\n",
    "    \n",
    "    # move start index (token level) to the token whose start index on character level matches answer start\n",
    "    while start_token < len(offset_mapping) and offset_mapping[start_token] is not None and start_char != offset_mapping[start_token][0]:\n",
    "        start_token += 1\n",
    "        \n",
    "    # move end index (token level) to the token whose end index on character level matches answer end\n",
    "    while end_token > 0 and offset_mapping[end_token] is not None and end_char != offset_mapping[end_token][1]:\n",
    "        end_token -= 1\n",
    "        \n",
    "    # check whether start_token is greater than end_token\n",
    "    # This could be the case if tokenization is not fine-grained enough, i.e. the answer does not start or ends with the first/last character of a token, but elsewhere within the span of a token \n",
    "    if start_token > end_token:\n",
    "        # return error result\n",
    "        return cls_index, cls_index, True, True\n",
    "    \n",
    "    return start_token, end_token, False, False\n",
    "\n",
    "def prepare_no_answerable_collection(sample_mapping):\n",
    "    \"\"\"\n",
    "    Prepares and returns a collection that contains for each original sample (indicated by its 'sample_index' in 'sample_mapping') an empty list.\n",
    "    This collection is used to collect tokenized samples for each original sample that are not answerable. \n",
    "    Note that the method returns 'None', if the feature of limiting 'no answerable samples' is disabled (see 'max_no_answers_per_possible_answer').\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    sample_mapping\n",
    "        List of indices where each item represents a tokenized samples and its value is the index of the original QA-sample\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Collection that contains for each original sample an empty list. Use the 'sample_index' to access the respective empty list.\n",
    "    The return value is 'None', if the feature of limiting 'no answerable samples' is disabled (see 'max_no_answers_per_possible_answer').\n",
    "    \"\"\"\n",
    "    \n",
    "    # create a collection that contains for each 'sample_index' an empty list (we will fill these lists with indices of 'no answerable samples')\n",
    "    if max_no_answers_per_possible_answer is not None:\n",
    "        # prepare a collection where each QA-sample has a list containg its no answerable tokenized samples\n",
    "        no_answerable_collection = dict()\n",
    "        \n",
    "        # iterate over all indices of QA-samples ...\n",
    "        for sample_index in sample_mapping:\n",
    "            if sample_index not in no_answerable_collection:\n",
    "                # ... and prepare an empty list\n",
    "                no_answerable_collection[sample_index] = []\n",
    "        return no_answerable_collection\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def pick_no_answerable_samples(no_answerable_collection, original_sample_index, tokenized_examples, question_id:str):\n",
    "    \"\"\"\n",
    "    Method randomly picks 'n' samples from the list in the passed no_answerable_collection that has the specified 'original_sample_index',\n",
    "    where 'n' has the value of 'max_no_answers_per_possible_answer'. The method removes the ignore flag in the passed 'tokenized_examples' collection for each picked sample and\n",
    "    adds the indicies of the picked samples to the global 'chosen_sample_indices' dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    no_answerable_collection \n",
    "        Collection that contains for each original sample a list with indices of no answerable samples. Use the method prepare_no_answerable_collection(...) to prepare this collection. \n",
    "        Make sure that this collection is not 'None' before calling this method.\n",
    "    original_sample_index : int\n",
    "        Index of the original QA-sample\n",
    "    tokenized_examples\n",
    "        Tokenized samples structure created by tokenizer\n",
    "    question_id : str\n",
    "        Question ID of the original QA-sample\n",
    "    \"\"\"\n",
    "    # If number of 'no answerable samples' is lower than number of samples to pick\n",
    "    if len(no_answerable_collection[original_sample_index]) < max_no_answers_per_possible_answer:\n",
    "        # pick them all\n",
    "        picked_indices = no_answerable_collection[original_sample_index]\n",
    "    else:\n",
    "        # else ... pick randomly 'no answerable samples'\n",
    "        picked_indices = random.sample(no_answerable_collection[original_sample_index],max_no_answers_per_possible_answer)\n",
    "\n",
    "    # For each (randomly) pick 'no answerable sample' \n",
    "    for picked_index in picked_indices: \n",
    "        # ... remove the 'ignore' flag\n",
    "        tokenized_examples[\"ignore\"][picked_index] = False\n",
    "\n",
    "    # Furthermore, in preparation for next iteration (epoch > 0), prepare a dictionary of chosen sample indices by memorizing their unique 'question_id' key of the original QA-sample and the list of samples indices of 'no answerable samples'\n",
    "    if question_id in chosen_sample_indices.keys(): \n",
    "        print(\"Warning! Collision detected for: \",question_id,\", sample index is \",original_sample_index)\n",
    "    chosen_sample_indices[question_id] = picked_indices\n",
    "\n",
    "def remove_ignore_flag_on_selected_answerable_samples(question_id: str, tokenized_examples):\n",
    "    \"\"\"\n",
    "    Removes the ignore flag in the passed 'tokenized_examples' collection for each no answerable sample whose index is stored in 'chosen_sample_indices' (global attribute) for the passed 'question_id'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    question_id : str\n",
    "        Question ID of the original QA-sample\n",
    "    tokenized_examples\n",
    "        Tokenized samples structure created by tokenizer\n",
    "    \"\"\"\n",
    "    for selected_index in chosen_sample_indices[question_id]:\n",
    "        # ... remove the 'ignore' flag\n",
    "        tokenized_examples[\"ignore\"][selected_index] = False\n",
    "                 \n",
    "def tokenize_training_samples(examples):\n",
    "    \n",
    "    global total_qa_samples\n",
    "    total_qa_samples+=len(examples[\"question\"])\n",
    "    \n",
    "    # Tokenizes the input dataset consisting of multiple QA-samples with truncation, padding, and overflow (stride).\n",
    "    # More precisely, each QA-sample may result into multiple tokenized samples if the combination of tokenized question and context exceeds the 'max_length'.\n",
    "    # In this case, the sub-context of the tokenized samples originating from the same QA-sample overlaps. In detail, the sub-context of the second tokenized sample starts with the last N token indices of the sub-context of the first sub-context and so further.\n",
    "    # So in fact, the sub-context of the subsequent tokenized sample overlaps a bit the sub-context of the previous tokenized sample.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # List of indices, where each item represents a tokenized sample and its value is the index of the original QA-sample, e.g. [0,0,1,2] which means that the first two tokenized samples belong to the first original QA-sample\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "    # prepare a collection that contains for each original sample an empty list (we will fill these lists with indices of 'no answerable samples')\n",
    "    no_answerable_collection = prepare_no_answerable_collection(sample_mapping)\n",
    "    \n",
    "    # Prepare output dataset structure\n",
    "    # 'start_positions' (int) contains the token index of the start of the answer for each tokenized sample. It value might be the 'cls_index' if its 'no answerable' sample.\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    # 'end_positions' (int) contains the token index of the end of the answer for each tokenized sample. It value might be the 'cls_index' if its 'no answerable' sample.\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    # Flag that indicates whether sample should be removed in the subsequent processing step (see remove_flagged_samples(...))\n",
    "    tokenized_examples[\"ignore\"] = []\n",
    "    \n",
    "    # Iterate over all tokenized samples and extract its offset_mapping\n",
    "    for i, offset_mapping in enumerate(tokenized_examples[\"offset_mapping\"]):\n",
    "        \n",
    "        # Mask offset mapping\n",
    "        masked_offset_mapping = mask_offset_mapping(\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i),\n",
    "            context_index = 1 if pad_on_right else 0,\n",
    "            offset_mapping = offset_mapping)\n",
    "        \n",
    "        # Load the index of the original QA-sample\n",
    "        sample_index = sample_mapping[i]\n",
    "        \n",
    "        # Load the answer\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        \n",
    "        # Load cls_index\n",
    "        cls_index = tokenized_examples[\"input_ids\"][i].index(tokenizer.cls_token_id)\n",
    "        \n",
    "        # Calculate start and end index (on token level)\n",
    "        start_index, end_index, answer_out_of_span, erroneous = calc_start_end_index(cls_index,masked_offset_mapping,answers)\n",
    "        \n",
    "        # skip if erroneous\n",
    "        if erroneous:\n",
    "            continue\n",
    "        \n",
    "        if answer_out_of_span:\n",
    "            # If number of 'no answerable samples' should be limited ...\n",
    "            if max_no_answers_per_possible_answer is not None:\n",
    "                # ... preliminarily flag tokenized sample as 'ignore'\n",
    "                tokenized_examples[\"ignore\"].append(True)\n",
    "                # ... add tokenized sample to 'no answerable samples' collection\n",
    "                no_answerable_collection[sample_index].append(i)\n",
    "            else:\n",
    "                # Else do nothing (do not ignore sample)\n",
    "                tokenized_examples[\"ignore\"].append(False)\n",
    "        else:\n",
    "            tokenized_examples[\"ignore\"].append(False) \n",
    "            \n",
    "        # append meta data to tokenized examples\n",
    "        tokenized_examples[\"start_positions\"].append(start_index)\n",
    "        tokenized_examples[\"end_positions\"].append(end_index)\n",
    "        tokenized_examples[\"offset_mapping\"][i] = masked_offset_mapping\n",
    "        \n",
    "        \n",
    "    # If number of 'no answerable samples' should be limited ...\n",
    "    if max_no_answers_per_possible_answer is not None:\n",
    "        # For each original QA-sample\n",
    "        for sample_index in no_answerable_collection.keys():\n",
    "            # If it is the first iteration (epoc), we will pick 'no answerable samples' randomly\n",
    "            if first_preprocessing_iteration:\n",
    "                pick_no_answerable_samples(no_answerable_collection,sample_index,tokenized_examples,examples[\"question_id\"][sample_index])\n",
    "            # If it is the second, third, ... iteration (epoch > 0), select 'no answerable samples' by their unique 'question_id' key\n",
    "            else:\n",
    "                remove_ignore_flag_on_selected_answerable_samples(examples[\"question_id\"][sample_index],tokenized_examples)\n",
    "                \n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f197341c-1eac-4206-bd69-8e4119bd5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_flagged_samples(examples):\n",
    "    \"\"\"\n",
    "    Removes all tokenized samples that have a ignore flag from the passed dataset and returns this dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : dict()\n",
    "        Tokenized samples\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tokenized samples without samples that have been marked as ignored\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    d[\"start_positions\"] = []\n",
    "    d[\"end_positions\"] = []\n",
    "    d[\"input_ids\"] = []\n",
    "    d[\"attention_mask\"] = []\n",
    "    \n",
    "    for i in range(len(examples[\"input_ids\"])):\n",
    "        if not examples[\"ignore\"][i]:\n",
    "            d[\"start_positions\"].append(examples[\"start_positions\"][i])\n",
    "            d[\"end_positions\"].append(examples[\"end_positions\"][i])\n",
    "            d[\"input_ids\"].append(examples[\"input_ids\"][i])\n",
    "            d[\"attention_mask\"].append(examples[\"attention_mask\"][i])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f256727-01ec-472f-bfdc-901d12e7e05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-423d6e444cb4ec4d\n"
     ]
    }
   ],
   "source": [
    "# Specifies the data source (i.e. the input) of our input pipeline. We will stream data without keeping loaded samples in memory\n",
    "dataset = load_dataset('json', data_files={'train':input_path+\"train/*.json\",'test':input_path+\"test/*.json\", 'validation':input_path+\"validation/*.json\"},streaming=True, keep_in_memory=None, cache_dir=caching_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63042704-bd9d-4ad1-8404-18f22eea053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connects the data source with expand_samples function\n",
    "expanded_dataset = dataset.map(expand_samples, batched=True, remove_columns=[\"questions\"], batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5699e732-e96f-4c12-b9bc-f8f720507a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connects the expanded results with the tokenizer\n",
    "tokenized_dataset = expanded_dataset.map(tokenize_training_samples, batched=True, remove_columns=[\"id\",\"title\",\"context\",\"question\",\"answers\",\"question_id\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d489cd0c-1fd4-4381-a77f-deaaa45af91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove flagged samples\n",
    "final_dataset = tokenized_dataset.map(remove_flagged_samples, batched=True, remove_columns=[\"ignore\",\"offset_mapping\"], batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afc01c07-45ad-491a-8c61-56eb97d283b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle dataset\n",
    "#final_dataset = removed_flag_dataset.shuffle(buffer_size=1024*8, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a87483-37b9-44dc-a639-68748c08cb99",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722fb0b9-d6de-4cce-83a6-f9bd109a0795",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626228f-a7d0-4a5c-922a-aa65a8ee03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_qa_samples = 0\n",
    "total_qa_samples_d = 0\n",
    "total_tokenized_samples = 0\n",
    "unanswerable_tokenized_samples = 0\n",
    "print_counter = 0\n",
    "\n",
    "\n",
    "for element in final_dataset[\"train\"]:\n",
    "\n",
    "    total_tokenized_samples+=1\n",
    "    print_counter+=1\n",
    "    \n",
    "    cls_index = element[\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "    if element[\"start_positions\"] == cls_index and element[\"end_positions\"] == cls_index:\n",
    "        unanswerable_tokenized_samples+=1\n",
    "    \n",
    "    if print_counter == 1000:\n",
    "        print_counter = 0\n",
    "        print(str(total_tokenized_samples/1000)+\"K\", end='\\r', flush=True)\n",
    "\n",
    "first_preprocessing_iteration = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c8670-034d-4aa4-95d4-d27bb7d90765",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#QA samples:\",total_qa_samples)\n",
    "print(\"#Tokenized samples:\",total_tokenized_samples)\n",
    "print(\"#Unanswerable tokenized samples:\", unanswerable_tokenized_samples,\" (\",(unanswerable_tokenized_samples/total_tokenized_samples)*100,\"%)\")\n",
    "print(\"#Answerable tokenized samples:\",total_tokenized_samples-unanswerable_tokenized_samples,\" (\",((total_tokenized_samples-unanswerable_tokenized_samples)/total_tokenized_samples)*100,\"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d9df9-63f7-4aba-9178-fd2b040d5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25289855-223f-4e52-9542-1d91fe61d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_steps = (total_tokenized_samples // batch_size) * num_train_epochs\n",
    "\n",
    "optimizer, schedule = create_optimizer(init_lr=learning_rate, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167324c9-afa5-4cff-992a-a01ac9227a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c2eb8e-63b3-4508-919d-8b7dc8513fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"\")\n",
    "wandb.init(project=project_name, name=run_name, config={\n",
    "    \"input_path\":input_path,\n",
    "    \"checkpoint_path\":checkpoint_path,\n",
    "    \"model_checkpoint\":model_checkpoint,\n",
    "    \"max_length\":max_length,\n",
    "    \"doc_stride\":doc_stride,\n",
    "    \"max_no_answers_per_possible_answer\":max_no_answers_per_possible_answer,\n",
    "    \"batch_size\":batch_size,\n",
    "    \"learning_rate\":learning_rate,\n",
    "    \"num_train_epochs\":num_train_epochs,\n",
    "    \"weight_decay\":weight_decay,\n",
    "    \"xla\":xla,\n",
    "    \"total_qa_samples\": total_qa_samples,\n",
    "    \"total_tokenized_samples\": total_tokenized_samples,\n",
    "    \"unanswerable_tokenized_samples\":unanswerable_tokenized_samples,\n",
    "    \"answerable_tokenized_samples\":total_tokenized_samples-unanswerable_tokenized_samples\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4906e9-60c4-43e2-87d7-c45555d40960",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3acfb2-4741-425a-a429-1cb242681def",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_train_epochs):\n",
    "    #print(\"Epoch \"+str(i+1)+\"/\"+str(num_train_epochs))\n",
    "    with tqdm(total=total_tokenized_samples // batch_size, desc=\"Epoch \"+str(i+1)+\"/\"+str(num_train_epochs)) as pbar:\n",
    "        total_step_counter = 0\n",
    "        batch_counter = 0\n",
    "        loss = 0\n",
    "        sample_counter_per_epoch = 0\n",
    "        \n",
    "        batch = dict() #added in V4.2\n",
    "        \n",
    "        for example in final_dataset[\"train\"]:\n",
    "            attention_mask_t = tf.constant([example[\"attention_mask\"]])\n",
    "            input_ids_t = tf.constant([example[\"input_ids\"]])\n",
    "            start_positions_t = tf.constant([example[\"start_positions\"]])\n",
    "            end_positions_t = tf.constant([example[\"end_positions\"]])\n",
    "      \n",
    "            if batch_counter == 0:\n",
    "                #batch = dict() #removed in V4.2\n",
    "                batch[\"attention_mask\"] = attention_mask_t\n",
    "                batch[\"input_ids\"] = input_ids_t\n",
    "                batch[\"start_positions\"] = start_positions_t\n",
    "                batch[\"end_positions\"] = end_positions_t\n",
    "            else:\n",
    "                batch[\"attention_mask\"] = tf.concat([batch[\"attention_mask\"], attention_mask_t],axis=0)\n",
    "                batch[\"input_ids\"] = tf.concat([batch[\"input_ids\"], input_ids_t],axis=0)\n",
    "                batch[\"start_positions\"] = tf.concat([batch[\"start_positions\"], start_positions_t],axis=0)\n",
    "                batch[\"end_positions\"] = tf.concat([batch[\"end_positions\"], end_positions_t],axis=0)\n",
    "      \n",
    "            batch_counter+=1\n",
    "            sample_counter_per_epoch+=1\n",
    "\n",
    "            if batch_counter == batch_size:\n",
    "                loss = model.train_on_batch(x=batch)\n",
    "                batch_counter = 0\n",
    "                total_step_counter+=1\n",
    "                pbar.update(1)\n",
    "                batch = dict() #added in V4.2\n",
    "        \n",
    "        # (added in V4.2) if there are untrained samples lef\n",
    "        if batch_counter > 0:\n",
    "            print(\"Final network update with remaining '\",batch_counter,\"' samples\")\n",
    "            loss = model.train_on_batch(x=batch)\n",
    "            total_step_counter+=1\n",
    "                \n",
    "        print(\"Loss \",loss)\n",
    "        wandb.log({\"loss\": loss, \"epoch\":(i+1), \"samples\": sample_counter_per_epoch})\n",
    "        \n",
    "        \n",
    "        #model.save(filepath=checkpoint_path+str(i)+\"_\"+str(loss))\n",
    "        model.save_pretrained(checkpoint_path+str(i)+\"_\"+str(loss))\n",
    "        print(\"Saving model at \"+checkpoint_path+str(i)+\"_\"+str(loss))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a18a7ec-acc0-4cca-9d1a-27f82b679cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hf]",
   "language": "python",
   "name": "conda-env-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
