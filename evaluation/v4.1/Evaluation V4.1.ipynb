{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec4cf94-a72b-4d93-8363-1a65fe5f370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes (v4):\n",
    "# - Bug fix: When creating batches, a few tokenized samples were omitted due to a missing 'append' statement and, therefore, not evaluated (thus, number of evaluated tokenized samples changed from 328115 to 328435)\n",
    "# - In 'evaluate_sample_batch(...)' the case 'or answer[\"property\"][\"name\"] == answer_text' has been added. \n",
    "# This is an edge case where the model predicts a property that is only partially contained in the presented context of the tokenized sample, but is the correct answer (although the tokenized sample has been marked as 'answer_out_of_span'\n",
    "\n",
    "# Changes (v4.1):\n",
    "# - option for verbose output when evaluating results added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e108bc-1592-40d8-bfee-be025488f068",
   "metadata": {},
   "source": [
    "# Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d8108a-fcab-4202-adbb-3210b473985f",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a64c87-c059-4992-b46c-f52f30dad03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734f543-6871-4044-b2e2-035281c96990",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c9574-4c1a-42bd-b835-7c745da2bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading the input dataset from disk\n",
    "from datasets import load_dataset\n",
    "from transformers import TFAutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "# for prediction (inference)\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b4fbb7-7344-4f71-9a38-126bd6d71b8b",
   "metadata": {},
   "source": [
    "## Check GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb675f-66d6-4e37-9f29-a476ca978208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TensorFlow version and whether GPU is available (there should be at least on physical device being listed)\n",
    "tf.__version__, tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d092458b-e410-480e-9f36-26f2c6e277a7",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24fc8f0-230e-4e37-8ed2-ae41244fa46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to directory where checkpoints should be loaded from\n",
    "checkpoint_directory = \"/home/user/directory_containing_best_checkpoint/\"\n",
    "\n",
    "# path to directory where evaluation results should be stored\n",
    "evaluation_path = \"/home/user/directory_for_evaluation_results/\"\n",
    "\n",
    "#path to directory where validation samples should be loaded from\n",
    "input_path = \"/home/user/directory_containing_rephrased_samples/\"\n",
    "\n",
    "# number of samples within one batch feed into network for inference\n",
    "batch_size = 1024\n",
    "# number of predicted answers per tokenized sample that should be considered, e.g., if a 'n_best_size' of 20 is chosen, only the top 20 answers with highest score are considered\n",
    "n_best_size = 20\n",
    "# optional (can be 'None') parameter that defines a maximum length (number of tokens) that an answer could have\n",
    "max_answer_length = None\n",
    "\n",
    "# optional (can be 'None') parameter that defines the maximum number of tokenized samples that should be evaluated (this parameter is used for debugging)\n",
    "limit = None\n",
    "\n",
    "# Parameters for tokenizing:\n",
    "max_length = 512  # Maximum number of features (i.e. indices) consisting of tokenized question and context in a tokenized sample\n",
    "doc_stride = 128  # Allowed overlap of the tokenized context if a QA-sample must be split into multiple tokenized samples due to length limitations\n",
    "\n",
    "# As a QA-sample might be split into multiple tokenized samples resulting into the situation that only one or a few tokenized samples contain the answer in its sub-context and the rest are no answerable samples,\n",
    "# we limit the number of no answerable samples by the following threshold parameter:\n",
    "max_no_answers_per_possible_answer = 3 # for each possible answer pick X no answer samples\n",
    "\n",
    "# Checkpoint of base model on Hugging Face\n",
    "base_model = \"microsoft/codebert-base\"\n",
    "\n",
    "# Flag indicating whether context should be shuffled before inference or not\n",
    "shuffle_context = False\n",
    "\n",
    "# Option for verbose output when evaluating results. If this flag is 'True', the notebook prints detail results for each QA-sample\n",
    "verbose_evaluation_output = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ecaa10-cc78-4df6-af20-6bd9e0f659f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for randomly picking X no answerable samples from all tokenized samples of a QA-sample\n",
    "random.seed(42)\n",
    "chosen_sample_indices = dict()\n",
    "first_preprocessing_iteration = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb8499-312a-4cd6-b0ee-f44e594cd862",
   "metadata": {},
   "source": [
    "# Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e64a6b-d4aa-4150-87e2-2a3f96aff467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "pad_on_right = tokenizer.padding_side == \"right\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0672f271-318d-4f7b-9ef5-c5e35d5d7e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_start(context: str,answer_text: str):\n",
    "    \"\"\"\n",
    "    Returns the position of the first character of the passed answer text in the specified context or None if the answer is not in the context.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    context : str\n",
    "        context\n",
    "    answer_text:\n",
    "        answer text\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Position of the first character of the answer text or None if the answer is not in the context\n",
    "    \"\"\"\n",
    "    \n",
    "    position = 0\n",
    "    for property in context.split():\n",
    "        if answer_text == property:\n",
    "            return position\n",
    "        else:\n",
    "            position += len(property)\n",
    "        position+=1 # for each space\n",
    "    return None\n",
    "\n",
    "def expand_samples(examples):\n",
    "    \"\"\"\n",
    "    'Data Preparation V3' has created a dataset where multiple question-answer pairs, each sharing same context, are collated within one sample. \n",
    "    This function expands the dataset structure so that in the resulting dataset each row represents a single question-answer pair.\n",
    "    Moreover, if enabled (see Settings), this function shuffles the context.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : dict()\n",
    "        Dictionary having the format:\n",
    "        {\n",
    "            'id': [string],\n",
    "            'title': [string],\n",
    "            'context': [string],\n",
    "            'questions':\n",
    "                [\n",
    "                    {\n",
    "                        'id': string\n",
    "                        'question': string\n",
    "                        'question_length': int\n",
    "                        'answers': \n",
    "                            {\n",
    "                                'text': [string],\n",
    "                                'answer_start': [int]\n",
    "                            }\n",
    "                    }\n",
    "                ]\n",
    "   \n",
    "        }\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Python dictionary having the format:\n",
    "    {\n",
    "        'id': [string],\n",
    "        'title': [string],\n",
    "        'context': [string],\n",
    "        'question': [string],\n",
    "        'question_id': [string],\n",
    "        'answers': \n",
    "            [\n",
    "                {\n",
    "                    'text': [string],\n",
    "                    'answer_start': [int]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "    }\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    d[\"id\"] = []\n",
    "    d[\"title\"] = []\n",
    "    d[\"context\"] = []\n",
    "    d[\"question\"] = []\n",
    "    d[\"question_id\"] = []\n",
    "    d[\"answers\"] = []\n",
    "  \n",
    "    for i in range(len(examples[\"id\"])):\n",
    "        for question in examples[\"questions\"][i]:\n",
    "            d[\"id\"].append(examples[\"id\"][i])\n",
    "            d[\"title\"].append(examples[\"title\"][i])\n",
    "            \n",
    "            context = \"\"\n",
    "            # optionally (see settings) shuffle context\n",
    "            if shuffle_context:\n",
    "                context = examples[\"context\"][i].split(\" \")\n",
    "                random.shuffle(context)\n",
    "                context = \" \".join(context)\n",
    "                d[\"context\"].append(context)\n",
    "            else:\n",
    "                context = examples[\"context\"][i]\n",
    "                d[\"context\"].append(examples[\"context\"][i])\n",
    "                \n",
    "            d[\"question\"].append(question[\"question\"])\n",
    "            d[\"question_id\"].append(question[\"id\"])\n",
    "            \n",
    "            # recalculate answer start as context might have been shuffled\n",
    "            question[\"answers\"][\"answer_start\"][0] = get_answer_start(context,question[\"answers\"][\"text\"][0])\n",
    "            d[\"answers\"].append(question[\"answers\"])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb74a8c-754d-4155-bd4d-2ffeec05efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_offset_mapping(sequence_ids, context_index, offset_mapping):\n",
    "    \"\"\"\n",
    "    Masks the passed 'offset_mapping', i.e. replaces all its entries with 'None' that represent tokens that are not part of the context.\n",
    "    The method returns the masked offset mapping.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence_ids :\n",
    "        Vector that defines for each token whether the token is part of the question, of the context, or is a special token.\n",
    "        An entry representing a token that is part of the context must have the value specified in 'context_index'\n",
    "    context_index : int\n",
    "        Value that is used to mark a token as part of the context in 'sequence_ids'\n",
    "    offset_mapping\n",
    "        Vector that contains for each token its start and end index on character level in the original input (consisting of question, context, and special characters). \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Masked 'offset_mapping'; entries that represent tokens that are not part of the context have the value 'None'\n",
    "    \"\"\"\n",
    "    masked_offset_mapping = [(o if sequence_ids[k] == context_index else None) for k, o in enumerate(offset_mapping)]\n",
    "    return masked_offset_mapping\n",
    "\n",
    "def calc_start_end_index(cls_index, offset_mapping, answers):\n",
    "    \"\"\"\n",
    "    Calculates start and end index on token level of the passed answer based on the specified 'offset_mapping'.\n",
    "    The method returns start and end index (token level) as well as flags indicating whether answer is out of span (third return parameter) and\n",
    "    whether the sample is erroneous since the calculated start index is greater than end index (fourth return parameter). If the answer is out of span,\n",
    "    start and end index have the value of the passed 'cls_index'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cls_index : int\n",
    "        Index of the [CLS] token that is used as start and end index if the answer is out of span\n",
    "    offset_mapping \n",
    "        Vector that contains for each token its start and end index on character level in the original input (consisting of question, context, and special characters). \n",
    "        All entries that represent a token that is not part of the context must be 'None' (use the method 'mask_offset_mapping(...)' to mask the 'offset_mapping' vector before using this method).\n",
    "    answers: dict()\n",
    "        Dictionary having the format:\n",
    "            {\n",
    "                'text': [string],\n",
    "                'answer_start': [int]\n",
    "            }\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    start_index : int\n",
    "        Calculated start index on token level (might have the value of 'cls_index' if the answer is out of span)\n",
    "    end_index : int\n",
    "        Calculated end index on token leven (might have the value of 'cls_index' if the answer is out of span)\n",
    "    answer_out_of_span : bool\n",
    "        Flag that indicates whether the answer is completely or partially out of span (True) or within span (False)\n",
    "    erroneous : bool\n",
    "        Flag that indicates whether the result of the calculation is erroneous. This might be the case if the answer \n",
    "        does not start or ends with the first/last character of a token, but elsewhere within the span of a token. \n",
    "    \"\"\"\n",
    "    \n",
    "    # if no answer is given\n",
    "    if not answers[\"answer_start\"]:\n",
    "        # return 'answer out of span' result\n",
    "        return cls_index, cls_index, True, False\n",
    "    \n",
    "    # load answer start (on character level)\n",
    "    start_char = answers[\"answer_start\"][0]\n",
    "    \n",
    "    # calculate answer end (on character level)\n",
    "    end_char = start_char + len(answers[\"text\"][0])\n",
    "    \n",
    "    # initialize start and end index (on token level)\n",
    "    start_token = 0\n",
    "    end_token = len(offset_mapping) - 1\n",
    "    \n",
    "    # move start index (token level) to the first token of the context\n",
    "    while offset_mapping[start_token] is None:\n",
    "        start_token += 1\n",
    "    \n",
    "    # move end index (token level) to the last token of the context\n",
    "    while offset_mapping[end_token] is None:\n",
    "        end_token -= 1\n",
    "        \n",
    "    # check whether answer is within context span:\n",
    "    # if start_char is smaller than index of first character of first context token OR end_char is greater than index of last character of last context token\n",
    "    if start_char < offset_mapping[start_token][0] or end_char > offset_mapping[end_token][1]:\n",
    "        # return 'answer out of span' result\n",
    "        return cls_index, cls_index, True, False\n",
    "    \n",
    "    # move start index (token level) to the token whose start index on character level matches answer start\n",
    "    while start_token < len(offset_mapping) and offset_mapping[start_token] is not None and start_char != offset_mapping[start_token][0]:\n",
    "        start_token += 1\n",
    "        \n",
    "    # move end index (token level) to the token whose end index on character level matches answer end\n",
    "    while end_token > 0 and offset_mapping[end_token] is not None and end_char != offset_mapping[end_token][1]:\n",
    "        end_token -= 1\n",
    "        \n",
    "    # check whether start_token is greater than end_token\n",
    "    # This could be the case if tokenization is not fine-grained enough, i.e. the answer does not start or ends with the first/last character of a token, but elsewhere within the span of a token \n",
    "    if start_token > end_token:\n",
    "        # return error result\n",
    "        return cls_index, cls_index, True, True\n",
    "    \n",
    "    return start_token, end_token, False, False\n",
    "\n",
    "def prepare_no_answerable_collection(sample_mapping):\n",
    "    \"\"\"\n",
    "    Prepares and returns a collection that contains for each original sample (indicated by its 'sample_index' in 'sample_mapping') an empty list.\n",
    "    This collection is used to collect tokenized samples for each original sample that are not answerable. \n",
    "    Note that the method returns 'None', if the feature of limiting 'no answerable samples' is disabled (see 'max_no_answers_per_possible_answer').\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    sample_mapping\n",
    "        List of indices where each item represents a tokenized samples and its value is the index of the original QA-sample\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Collection that contains for each original sample an empty list. Use the 'sample_index' to access the respective empty list.\n",
    "    The return value is 'None', if the feature of limiting 'no answerable samples' is disabled (see 'max_no_answers_per_possible_answer').\n",
    "    \"\"\"\n",
    "    \n",
    "    # create a collection that contains for each 'sample_index' an empty list (we will fill these lists with indices of 'no answerable samples')\n",
    "    if max_no_answers_per_possible_answer is not None:\n",
    "        # prepare a collection where each QA-sample has a list containg its no answerable tokenized samples\n",
    "        no_answerable_collection = dict()\n",
    "        \n",
    "        # iterate over all indices of QA-samples ...\n",
    "        for sample_index in sample_mapping:\n",
    "            if sample_index not in no_answerable_collection:\n",
    "                # ... and prepare an empty list\n",
    "                no_answerable_collection[sample_index] = []\n",
    "        return no_answerable_collection\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def pick_no_answerable_samples(no_answerable_collection, original_sample_index, tokenized_examples, question_id:str):\n",
    "    \"\"\"\n",
    "    Method randomly picks 'n' samples from the list in the passed no_answerable_collection that has the specified 'original_sample_index',\n",
    "    where 'n' has the value of 'max_no_answers_per_possible_answer'. The method removes the ignore flag in the passed 'tokenized_examples' collection for each picked sample and\n",
    "    adds the indicies of the picked samples to the global 'chosen_sample_indices' dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    no_answerable_collection \n",
    "        Collection that contains for each original sample a list with indices of no answerable samples. Use the method prepare_no_answerable_collection(...) to prepare this collection. \n",
    "        Make sure that this collection is not 'None' before calling this method.\n",
    "    original_sample_index : int\n",
    "        Index of the original QA-sample\n",
    "    tokenized_examples\n",
    "        Tokenized samples structure created by tokenizer\n",
    "    question_id : str\n",
    "        Question ID of the original QA-sample\n",
    "    \"\"\"\n",
    "    # If number of 'no answerable samples' is lower than number of samples to pick\n",
    "    if len(no_answerable_collection[original_sample_index]) < max_no_answers_per_possible_answer:\n",
    "        # pick them all\n",
    "        picked_indices = no_answerable_collection[original_sample_index]\n",
    "    else:\n",
    "        # else ... pick randomly 'no answerable samples'\n",
    "        picked_indices = random.sample(no_answerable_collection[original_sample_index],max_no_answers_per_possible_answer)\n",
    "\n",
    "    # For each (randomly) pick 'no answerable sample' \n",
    "    for picked_index in picked_indices: \n",
    "        # ... remove the 'ignore' flag\n",
    "        tokenized_examples[\"ignore\"][picked_index] = False\n",
    "\n",
    "    # Furthermore, in preparation for next iteration (epoch > 0), prepare a dictionary of chosen sample indices by memorizing their unique 'question_id' key of the original QA-sample and the list of samples indices of 'no answerable samples'\n",
    "    if question_id in chosen_sample_indices.keys(): \n",
    "        print(\"Warning! Collision detected for: \",question_id,\", sample index is \",original_sample_index)\n",
    "    chosen_sample_indices[question_id] = picked_indices\n",
    "\n",
    "def remove_ignore_flag_on_selected_answerable_samples(question_id: str, tokenized_examples):\n",
    "    \"\"\"\n",
    "    Removes the ignore flag in the passed 'tokenized_examples' collection for each no answerable sample whose index is stored in 'chosen_sample_indices' (global attribute) for the passed 'question_id'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    question_id : str\n",
    "        Question ID of the original QA-sample\n",
    "    tokenized_examples\n",
    "        Tokenized samples structure created by tokenizer\n",
    "    \"\"\"\n",
    "    for selected_index in chosen_sample_indices[question_id]:\n",
    "        # ... remove the 'ignore' flag\n",
    "        tokenized_examples[\"ignore\"][selected_index] = False\n",
    "                 \n",
    "def tokenize_validation_samples(examples):\n",
    "    \n",
    "    # Tokenizes the input dataset consisting of multiple QA-samples with truncation, padding, and overflow (stride).\n",
    "    # More precisely, each QA-sample may result into multiple tokenized samples if the combination of tokenized question and context exceeds the 'max_length'.\n",
    "    # In this case, the sub-context of the tokenized samples originating from the same QA-sample overlaps. In detail, the sub-context of the second tokenized sample starts with the last N token indices of the sub-context of the first sub-context and so further.\n",
    "    # So in fact, the sub-context of the subsequent tokenized sample overlaps a bit the sub-context of the previous tokenized sample.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # List of indices, where each item represents a tokenized sample and its value is the index of the original QA-sample, e.g. [0,0,1,2] which means that the first two tokenized samples belong to the first original QA-sample\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "    # prepare a collection that contains for each original sample an empty list (we will fill these lists with indices of 'no answerable samples')\n",
    "    no_answerable_collection = prepare_no_answerable_collection(sample_mapping)\n",
    "    \n",
    "    # Prepare output dataset structure\n",
    "    # 'start_positions' (int) contains the token index of the start of the answer for each tokenized sample. It value might be the 'cls_index' if its 'no answerable' sample.\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    # 'end_positions' (int) contains the token index of the end of the answer for each tokenized sample. It value might be the 'cls_index' if its 'no answerable' sample.\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    \n",
    "    # ID of the sample (string), e.g. \"7fed77b9abe24a2db869c8b9919a1e9b\"\n",
    "    tokenized_examples[\"original_sample_id\"] = []\n",
    "    # Context of the sample (string), e.g. \"users[*].id users[*].name _links.href _links.rel\"\n",
    "    tokenized_examples[\"original_context\"] = []\n",
    "    # Answer (string) of the sample, e.g. \"users[*].name\"\n",
    "    tokenized_examples[\"original_answer_text\"] = []\n",
    "    # Start index of the answer of the sample (int), e.g. 12\n",
    "    tokenized_examples[\"original_answer_start\"] = []\n",
    "    # Question of the sample (string), e.g. \"The name of a user\"\n",
    "    tokenized_examples[\"original_question\"] = []\n",
    "    # ID of the question (string), e.g. \"7fed77b9abe24a2db869c8b9919a1e9b_6sasd7b9abe24a2db869c8b9919a1e9b\" (syntax: ID_Question ID)\n",
    "    tokenized_examples[\"original_question_id\"] = []\n",
    "    # Number of tokens of the question of the sample (int), e.g. 5\n",
    "    #tokenized_examples[\"original_question_length\"] = []\n",
    "    # Flag whether answer is out of span (i.e. not contain within truncated context) (boolean)\n",
    "    tokenized_examples[\"answer_out_of_span\"] = []\n",
    "    # Index of the CLS token (int) \n",
    "    tokenized_examples[\"cls_index\"] = []\n",
    "    # Flag that indicates whether sample should be removed in the subsequent processing step (see remove_flagged_samples(...))\n",
    "    tokenized_examples[\"ignore\"] = []\n",
    "    \n",
    "    # Iterate over all tokenized samples and extract its offset_mapping\n",
    "    for i, offset_mapping in enumerate(tokenized_examples[\"offset_mapping\"]):\n",
    "        \n",
    "        # Mask offset mapping\n",
    "        masked_offset_mapping = mask_offset_mapping(\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i),\n",
    "            context_index = 1 if pad_on_right else 0,\n",
    "            offset_mapping = offset_mapping)\n",
    "        \n",
    "        # Load the index of the original QA-sample\n",
    "        sample_index = sample_mapping[i]\n",
    "        \n",
    "        # Load the answer\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        \n",
    "        # Load cls_index\n",
    "        cls_index = tokenized_examples[\"input_ids\"][i].index(tokenizer.cls_token_id)\n",
    "        \n",
    "        # Calculate start and end index (on token level)\n",
    "        start_index, end_index, answer_out_of_span, erroneous = calc_start_end_index(cls_index,masked_offset_mapping,answers)\n",
    "        \n",
    "        # skip if erroneous\n",
    "        if erroneous:\n",
    "            continue\n",
    "        \n",
    "        if answer_out_of_span:\n",
    "            # If number of 'no answerable samples' should be limited ...\n",
    "            if max_no_answers_per_possible_answer is not None:\n",
    "                # ... preliminarily flag tokenized sample as 'ignore'\n",
    "                tokenized_examples[\"ignore\"].append(True)\n",
    "                # ... add tokenized sample to 'no answerable samples' collection\n",
    "                no_answerable_collection[sample_index].append(i)\n",
    "            else:\n",
    "                # Else do nothing (do not ignore sample)\n",
    "                tokenized_examples[\"ignore\"].append(False)\n",
    "        else:\n",
    "            tokenized_examples[\"ignore\"].append(False) \n",
    "            \n",
    "        # append meta data to tokenized examples\n",
    "        tokenized_examples[\"original_sample_id\"].append(examples[\"id\"][sample_index])\n",
    "        tokenized_examples[\"original_context\"].append(examples[\"context\"][sample_index])\n",
    "        tokenized_examples[\"original_answer_text\"].append(examples[\"answers\"][sample_index][\"text\"][0])\n",
    "        tokenized_examples[\"original_answer_start\"].append(examples[\"answers\"][sample_index][\"answer_start\"][0])\n",
    "        tokenized_examples[\"original_question\"].append(examples[\"question\"][sample_index])\n",
    "        tokenized_examples[\"original_question_id\"].append(examples[\"question_id\"][sample_index])\n",
    "        tokenized_examples[\"start_positions\"].append(start_index)\n",
    "        tokenized_examples[\"end_positions\"].append(end_index)\n",
    "        tokenized_examples[\"cls_index\"].append(cls_index)\n",
    "        tokenized_examples[\"answer_out_of_span\"].append(answer_out_of_span) \n",
    "        tokenized_examples[\"offset_mapping\"][i] = masked_offset_mapping\n",
    "        \n",
    "        \n",
    "    # If number of 'no answerable samples' should be limited ...\n",
    "    if max_no_answers_per_possible_answer is not None:\n",
    "        # For each original QA-sample\n",
    "        for sample_index in no_answerable_collection.keys():\n",
    "            # If it is the first iteration (epoc), we will pick 'no answerable samples' randomly\n",
    "            if first_preprocessing_iteration:\n",
    "                pick_no_answerable_samples(no_answerable_collection,sample_index,tokenized_examples,examples[\"question_id\"][sample_index])\n",
    "            # If it is the second, third, ... iteration (epoch > 0), select 'no answerable samples' by their unique 'question_id' key\n",
    "            else:\n",
    "                remove_ignore_flag_on_selected_answerable_samples(examples[\"question_id\"][sample_index],tokenized_examples)\n",
    "                \n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dca1da8-f7e5-4808-9faf-d2c8de319940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_flagged_samples(examples):\n",
    "    \"\"\"\n",
    "    Removes all tokenized samples that have a ignore flag from the passed dataset and returns this dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : dict()\n",
    "        Tokenized samples\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tokenized samples without samples that have been marked as ignored\n",
    "    \"\"\"\n",
    "    \n",
    "    d = {}\n",
    "    d[\"start_positions\"] = []\n",
    "    d[\"end_positions\"] = []\n",
    "    d[\"input_ids\"] = []\n",
    "    d[\"attention_mask\"] = []\n",
    "    d[\"offset_mapping\"] = []\n",
    "    \n",
    "    # ID of the sample (string), e.g. \"7fed77b9abe24a2db869c8b9919a1e9b\"\n",
    "    d[\"original_sample_id\"] = []\n",
    "    # Context of the sample (string), e.g. \"users[*].id users[*].name _links.href _links.rel\"\n",
    "    d[\"original_context\"] = []\n",
    "    # Answer (string) of the sample, e.g. \"users[*].name\"\n",
    "    d[\"original_answer_text\"] = []\n",
    "    # Start index of the answer of the sample (int), e.g. 12\n",
    "    d[\"original_answer_start\"] = []\n",
    "    # Question of the sample (string), e.g. \"The name of a user\"\n",
    "    d[\"original_question\"] = []\n",
    "    # ID of the question (string), e.g. \"7fed77b9abe24a2db869c8b9919a1e9b_6sasd7b9abe24a2db869c8b9919a1e9b\" (syntax: ID_Question ID) \n",
    "    d[\"original_question_id\"] = []\n",
    "    # Flag whether answer is out of scope (i.e. not contain within truncated context) (boolean)\n",
    "    d[\"answer_out_of_span\"] = []\n",
    "    # Index of the CLS token (int) \n",
    "    d[\"cls_index\"] = []\n",
    "    # Flag that indicates whether sample should be removed in the subsequent processing step (see remove_flagged_samples(...))\n",
    "    \n",
    "    for i in range(len(examples[\"input_ids\"])):\n",
    "        if not examples[\"ignore\"][i]:\n",
    "            d[\"start_positions\"].append(examples[\"start_positions\"][i])\n",
    "            d[\"end_positions\"].append(examples[\"end_positions\"][i])\n",
    "            d[\"input_ids\"].append(examples[\"input_ids\"][i])\n",
    "            d[\"attention_mask\"].append(examples[\"attention_mask\"][i])\n",
    "            d[\"offset_mapping\"].append(examples[\"offset_mapping\"][i])\n",
    "            d[\"original_sample_id\"].append(examples[\"original_sample_id\"][i])\n",
    "            d[\"original_context\"].append(examples[\"original_context\"][i])\n",
    "            d[\"original_answer_text\"].append(examples[\"original_answer_text\"][i])\n",
    "            d[\"original_answer_start\"].append(examples[\"original_answer_start\"][i])\n",
    "            d[\"original_question\"].append(examples[\"original_question\"][i])\n",
    "            d[\"original_question_id\"].append(examples[\"original_question_id\"][i])\n",
    "            d[\"answer_out_of_span\"].append(examples[\"answer_out_of_span\"][i])\n",
    "            d[\"cls_index\"].append(examples[\"cls_index\"][i])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a68a8-13a8-425f-b9a2-6dbe71ca0b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation data set:\n",
    "validation_dataset = load_dataset('json', data_files={'validation':input_path+\"validation/*.json\"}, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ad078-b90f-4fa1-bde9-754bf705f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connects the data source with expand_samples function\n",
    "expanded_dataset = validation_dataset[\"validation\"].map(expand_samples, batched=True, remove_columns=[\"questions\"], batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64860166-0499-4cdc-9323-d301ff3a9e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connects the expanded results with the tokenizer\n",
    "tokenized_dataset = expanded_dataset.map(tokenize_validation_samples, batched=True, remove_columns=[\"id\",\"title\",\"context\",\"question\",\"answers\",\"question_id\"], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b0336b-eb99-4dc7-bfac-d438c866da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizes the dataset\n",
    "final_dataset = tokenized_dataset.map(remove_flagged_samples, batched=True, remove_columns=[\"ignore\"], batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40077de3-017c-4c38-9129-01d6bf2ff02b",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79582664-e39a-455d-a5b4-d7cee9c79a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_properties(context,start_char_index,end_char_index):\n",
    "    \"\"\"\n",
    "    Identifies the properties that are (partially) covered by the span starting at 'start_char_index' and ending at 'end_char_index' in the specified context.\n",
    "    The method returns the list of properties that are fully or partially covered. \n",
    "    Each property contained in this list is a dictionary and has the following structure:\n",
    "    {\n",
    "        'name': full property name in XPath style (str)\n",
    "        'partial_name': concatenated characters of the property name that are covered (str)\n",
    "        'length': number of characters of the property that are covered (int)\n",
    "        'partial': flag indicating whether property is fully (False) or partially (True) covered\n",
    "        'start_char_index': start index of the property on character level in the context (int)\n",
    "        'end_char_index': end index of the property on character level in the context (int)\n",
    "    }\n",
    "    Note: If the property is only partially covered, the fields 'start_char_index' and 'end_char_index' point to the start and end of the partial, not the full property\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    context : str\n",
    "        Original context\n",
    "    start_char_index : int\n",
    "        Start index of the span on character level in the original context\n",
    "    end_char_index : int\n",
    "        End index of the span on character level in the original context\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List of identified properties\n",
    "    \"\"\"\n",
    "    properties = []\n",
    "    current_property = None\n",
    "    is_on_property = False\n",
    "    \n",
    "    # iterate over each character of the span\n",
    "    for i in range(end_char_index-start_char_index):\n",
    "        index = start_char_index+i\n",
    "        c = context[index]\n",
    "       \n",
    "        # if character is a property separator\n",
    "        if c == \" \":\n",
    "            # if there is still an unfinished property\n",
    "            if is_on_property:\n",
    "                # finalize property\n",
    "                current_property[\"end_char_index\"] = index\n",
    "                \n",
    "                # if start index of property is start index of whole span (answer), i.e. there are characters belonging to the property that are before span\n",
    "                if current_property[\"start_char_index\"] == start_char_index:\n",
    "                    # go backward in context and determine full property name\n",
    "                    back_counter = start_char_index-1\n",
    "                    while back_counter >= 0 and context[back_counter] != \" \":\n",
    "                        current_property[\"name\"] = context[back_counter] + current_property[\"name\"]\n",
    "                        current_property[\"partial\"] = True # only True (i.e partial), if we have to go backward\n",
    "                        back_counter-=1\n",
    "                properties.append(current_property)\n",
    "                current_property = None\n",
    "            is_on_property = False\n",
    "        else:\n",
    "            # if this is the first character of a new property\n",
    "            if not is_on_property:\n",
    "                # prepare a new property\n",
    "                current_property = {\n",
    "                    \"name\": c,\n",
    "                    \"partial_name\": c,\n",
    "                    \"length\": 1,\n",
    "                    \"partial\": False,\n",
    "                    \"start_char_index\": index,\n",
    "                    \"end_char_index\": None\n",
    "                }\n",
    "            else:\n",
    "                # else, append character to current property\n",
    "                current_property[\"name\"]+= c\n",
    "                current_property[\"partial_name\"]+= c\n",
    "                current_property[\"length\"]+=1\n",
    "            \n",
    "            # in both cases (either a new or an existing one), we are on a property \n",
    "            is_on_property = True\n",
    "    \n",
    "    # after iterating over all characters of span\n",
    "    # check whether there is an unfinished property:\n",
    "    if current_property:\n",
    "        current_property[\"end_char_index\"] = end_char_index\n",
    "        \n",
    "        # go forward in context and determine full property name\n",
    "        forward_counter = end_char_index\n",
    "        while forward_counter < len(context) and context[forward_counter] != \" \":\n",
    "            current_property[\"name\"] = current_property[\"name\"] + context[forward_counter]\n",
    "            current_property[\"partial\"] = True # only True (i.e partial), if we have to go forward\n",
    "            forward_counter+=1\n",
    "            \n",
    "        \n",
    "        # Special case: If the unfinished property is the only identified property (i.e. properties is empty),\n",
    "        # we had not the chance to go backward yet (since going backward is only possible, when finalizing a property, see code above)\n",
    "        # Thus go backward in context and determine full property name\n",
    "        if len(properties) == 0:\n",
    "            back_counter = start_char_index-1\n",
    "            while back_counter >= 0 and context[back_counter] != \" \":\n",
    "                current_property[\"name\"] = context[back_counter] + current_property[\"name\"]\n",
    "                current_property[\"partial\"] = True # only True (i.e partial), if we have to go backward\n",
    "                back_counter-=1\n",
    "                \n",
    "        properties.append(current_property)\n",
    "        \n",
    "    return properties\n",
    "\n",
    "def determine_best_property(properties):\n",
    "    \"\"\"\n",
    "    Determines the 'best' property from the passed list of properties. First, the method scans the input list for properties that are fully covered.\n",
    "    Only if exactly one fully covered property exists, then the method determines and returns this properties as the best property. If two or more\n",
    "    fully covered properties exist, the method will return 'None' due to this conflict. If the list does not contain any fully covered property,\n",
    "    the method scans for partial properties in step two: The method determines and returns the partial covered property with the longest sequence of\n",
    "    covered characters. If multiple partial covered properties share this longest sequence (i.e. having the same length), the method will return 'None'\n",
    "    due to this conflict. Furthermore, the method returns 'None' if the input list is empty.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    properties : [dict()]\n",
    "        List of properties (use identify_properties(...) to identify these properties)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The 'best' property or 'None'\n",
    "    \"\"\"\n",
    "    if not properties:\n",
    "        # no properties\n",
    "        return None\n",
    "    else:\n",
    "        # first, search for full properties\n",
    "        full_property = None\n",
    "        for p in properties:\n",
    "            if not p[\"partial\"]:\n",
    "                if full_property is None:\n",
    "                    full_property = p\n",
    "                else:\n",
    "                    # at least two full properties (--> conflict)\n",
    "                    return None\n",
    "        if full_property:\n",
    "            # one full property\n",
    "            return full_property\n",
    "        else:\n",
    "            # then, search for partial properties\n",
    "            partial_property = None\n",
    "            length_conflict = False\n",
    "            for p in properties:\n",
    "                if p[\"partial\"]:\n",
    "                    if partial_property is None:\n",
    "                        partial_property = p\n",
    "                    else:\n",
    "                        if partial_property[\"length\"] < p[\"length\"]:\n",
    "                            partial_property = p\n",
    "                            length_conflict = False\n",
    "                        elif partial_property[\"length\"] == p[\"length\"]:\n",
    "                            length_conflict = True\n",
    "            \n",
    "            if length_conflict:\n",
    "                # two longest partial properties have same length (--> conflict)\n",
    "                return None\n",
    "            else:\n",
    "                return partial_property\n",
    "            \n",
    "def predict(batched_samples, model):\n",
    "    \"\"\"\n",
    "    Converts the passed batch of tokenized samples into a tensor that is feed into the passed transformer model for prediction.\n",
    "    The method returns the model's output as well as the number of input samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batched_samples : [dict()]\n",
    "        Batch of samples where each sample is a dictionary have the fields 'attention_mask' and 'input_ids'\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    The output of the model (first return parameter) and the number of input samples (second return parameter)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_counter = 0\n",
    "    for sample in batched_samples:\n",
    "        # create input tensor for each sample of batch:\n",
    "        # attention mask is a binary tensor so that the model knows to which token it has to attend to (typically 0 for padded indices)\n",
    "        attention_mask_t = tf.constant([sample[\"attention_mask\"]])\n",
    "        input_ids_t = tf.constant([sample[\"input_ids\"]])\n",
    "        \n",
    "        if batch_counter == 0:\n",
    "            batch = dict()\n",
    "            batch[\"attention_mask\"] = attention_mask_t\n",
    "            batch[\"input_ids\"] = input_ids_t\n",
    "        else:\n",
    "            batch[\"attention_mask\"] = tf.concat([batch[\"attention_mask\"], attention_mask_t],axis=0)\n",
    "            batch[\"input_ids\"] = tf.concat([batch[\"input_ids\"],input_ids_t],axis=0)\n",
    "        batch_counter+=1\n",
    "    \n",
    "    output = model.predict(batch, verbose=0) #with default batch_size=32 (see Tensorflow 2.10)\n",
    "    return output, batch_counter\n",
    "\n",
    "\n",
    "def are_indices_out_of_context(start_index, end_index, offset_mapping):\n",
    "    \"\"\"\n",
    "    Returns 'True' if the span defined by the passed start and end index (token level) does not completely lies within the context, else 'False'.\n",
    "    The boundaries of the context (i.e. start and end index) are defined within the passed offset_mapping: \n",
    "    Every entry that represents a token that is not part of the context has the value 'None'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    start_index : int\n",
    "        Start index (on token level) of the span\n",
    "    end_index : int\n",
    "        End index (on token level) of the span\n",
    "    offset_mapping\n",
    "        Vector that contains for each token its start and end index on character level in the original input (consisting of question, context, and special characters). \n",
    "        All entries that represent a token that is not part of the context must be 'None' (use the method 'mask_offset_mapping(...)' to mask the 'offset_mapping' vector before using this method).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    'True' if the span does not completely lies within the context, else 'False'\n",
    "    \"\"\"\n",
    "    return (start_index >= len(offset_mapping) or end_index >= len(offset_mapping) #if indices are out of bound (should never happen????)\n",
    "            or offset_mapping[start_index] is None or offset_mapping[end_index] is None)\n",
    "\n",
    "def is_end_before_start(start_index, end_index):\n",
    "    \"\"\"\n",
    "    Returns 'True' if the passed 'end_index' is smaller than (i.e. before) the passed 'start_index', else 'False'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    start_index : int\n",
    "        Start index (on token level) of the span\n",
    "    end_index : int\n",
    "        End index (on token level) of the span\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    'True' if the passed 'end_index' is smaller than (i.e. before) the passed 'start_index', else 'False'.\n",
    "    \"\"\"\n",
    "    return end_index < start_index\n",
    "\n",
    "def is_answer_too_long(start_index, end_index,max_length):\n",
    "    \"\"\"\n",
    "    Returns 'True', if the number of tokens in the span defined by the passed start and end index (token level) exceeds the passed length, else 'False'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    start_index : int\n",
    "        Start index (on token level) of the span\n",
    "    end_index : int\n",
    "        End index (on token level) of the span\n",
    "    max_length : int\n",
    "        Maximum length\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    'True', if the number of tokens in the span exceeds the passed length, else 'False'.\n",
    "    \"\"\"\n",
    "    if max_length:\n",
    "        return end_index - start_index + 1 > max_length\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def get_answers(sample, predicted_start_logits, predicted_end_logits):\n",
    "    offset_mapping = sample[\"offset_mapping\"]\n",
    "    cls_index = sample[\"cls_index\"]\n",
    "    context = sample[\"original_context\"]\n",
    "    \n",
    "    # Gather the indices for the best start/end logits (index syntax is: [stop:start:steps] with steps = -1 --> negative order)\n",
    "    # np.argsort returns a sorted list of indices in ascending order, therefore, we gather the last 'n_best_size' indices\n",
    "    # in reverse order (syntax: [stop:start:steps] with steps = -1 --> negative order)\n",
    "    #(see https://towardsdatascience.com/the-basics-of-indexing-and-slicing-python-lists-2d12c90a94cf)\n",
    "    best_start_indices = np.argsort(predicted_start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "    best_end_indices = np.argsort(predicted_end_logits)[-1 : -n_best_size - 1 : -1].tolist()   \n",
    "    \n",
    "    # prepare list for answers\n",
    "    valid_answers = []\n",
    "    \n",
    "    for start_index in best_start_indices:\n",
    "        for end_index in best_end_indices:\n",
    "            # Do not consider....\n",
    "            \n",
    "            # Case 1:) Answers that are out of context\n",
    "            # In this case, either start_index or end_index (or both) point to a token positions outside the context\n",
    "            # Remember: We have set all positions of tokens, which are out of context, with 'None' in \"offset_mapping\" (see tokenize_validation_samples)\n",
    "            if are_indices_out_of_context(start_index,end_index,offset_mapping):\n",
    "                continue\n",
    "                \n",
    "            # Case 2:) Answers where end is before start index\n",
    "            if is_end_before_start(start_index, end_index):\n",
    "                continue\n",
    "            \n",
    "            # Optional case 3:) Answers that are too long\n",
    "            if is_answer_too_long(start_index, end_index,max_answer_length):\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            start_char_index = offset_mapping[start_index][0]\n",
    "            end_char_index = offset_mapping[end_index][1]\n",
    "            \n",
    "            # identify properties and determine best property\n",
    "            properties = identify_properties(context,start_char_index,end_char_index)\n",
    "            best_property = determine_best_property(properties)\n",
    "            \n",
    "            # Case 4:) Answers that do not point clearly to a property (without conflicts)\n",
    "            if best_property is None:\n",
    "                continue\n",
    "            \n",
    "            # add answer:\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": predicted_start_logits[start_index] + predicted_end_logits[end_index],\n",
    "                    \"span\": context[start_char_index:end_char_index],\n",
    "                    \"start_char_index\": start_char_index,\n",
    "                    \"end_char_index\": end_char_index,\n",
    "                    \"property\": best_property\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    # finally, add NULL answer as valid answer\n",
    "    valid_answers.append(\n",
    "        {\n",
    "            \"score\": predicted_start_logits[cls_index] + predicted_end_logits[cls_index],\n",
    "            \"span\": None,\n",
    "            \"start_char_index\": cls_index,\n",
    "            \"end_char_index\": cls_index,\n",
    "            \"property\": None\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # sort valid answers by score in descending order\n",
    "    sorted_valid_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)\n",
    "    return sorted_valid_answer\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6c1fb-e931-4bc7-b9a7-1d043e0962e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sample_batch(batched_samples, model):\n",
    "    predictions = []\n",
    "    \n",
    "    output, batch_size = predict(batched_samples, model)\n",
    "    \n",
    "    # for each tokenized sample of the batch...\n",
    "    for i in range(batch_size):\n",
    "        # ... we create a prediction object (see comments below)\n",
    "        \n",
    "        # load sample from batch and predicted start and end logits\n",
    "        sample = batched_samples[i]\n",
    "        start_logits = output.start_logits[i]\n",
    "        end_logits = output.end_logits[i]\n",
    "        \n",
    "        # load correct answer of original QA sample and the flag indicating whether this correct answer is out of span for the tokenized sample\n",
    "        answer_out_of_span = sample[\"answer_out_of_span\"]\n",
    "        answer_text = sample[\"original_answer_text\"]\n",
    "        \n",
    "        # Interpret start and end logits (output of the model). get_answers returns a ranked list of predicted answers\n",
    "        answers = get_answers(sample, start_logits, end_logits)\n",
    "        \n",
    "        # prepare prediction object with:\n",
    "        prediction = {\n",
    "            # the ID of the QA sample (only for documentation) #TODO: better use original_question_id here (however, does not affect results)\n",
    "            \"id\": sample[\"original_sample_id\"],\n",
    "            # the correct answer of the entire QA sample\n",
    "            \"theoretical_answer\": answer_text,\n",
    "            # flag indicating whether the correct answer of the entire QA sample is out of span for this tokenized sample\n",
    "            \"theoretical_answer_out_of_span\": answer_out_of_span,\n",
    "            # the rank (i.e. position) of the correct answer in the ranked list of predicted answers (will be determined in the following)\n",
    "            \"rank\": None,\n",
    "            # the predicted answer that matches the correct answer (will be determined in the following)\n",
    "            \"correct_answer\": None,\n",
    "            # the ranked list of predicted answers\n",
    "            \"answers\": answers\n",
    "        }\n",
    "        \n",
    "        if verbose_evaluation_output:\n",
    "            print(\"Question ID\",sample[\"original_question_id\"])\n",
    "            print(\"Question\",sample[\"original_question\"])\n",
    "            print(\"Correct Answer\",answer_text)\n",
    "        \n",
    "        \n",
    "        # determine correct answer by iterating ranked list of predicted answers (starting with the higest ranked answer)\n",
    "        for i, answer in enumerate(answers):\n",
    "            if verbose_evaluation_output:\n",
    "                if answer[\"property\"] is not None: \n",
    "                    print(\"Answer [\"+str(i)+\"]: \"+answer[\"property\"][\"name\"])\n",
    "                else:\n",
    "                    print(\"Answer [\"+str(i)+\"]: <   >\")\n",
    "            \n",
    "            # if correct answer for tokenized sample is out of span\n",
    "            if answer_out_of_span:\n",
    "                # and if predicted answer is out of span as well\n",
    "                # 2023-04-18 (v4): 'or answer[\"property\"][\"name\"] == answer_text' added. This is an edge case where the model predicts a property that is only partially contained in the presented context of the tokenized sample, but is the correct answer (although the tokenized sample has been marked as 'answer_out_of_span'\n",
    "                if answer[\"property\"] is None or answer[\"property\"][\"name\"] == answer_text:\n",
    "                    prediction[\"correct_answer\"] = answer\n",
    "                    prediction[\"rank\"] = i+1\n",
    "                    break\n",
    "            # if correct answer for tokenized sample is NOT out of span\n",
    "            else:\n",
    "                # and if predicted answer is equals correct answer\n",
    "                if answer[\"property\"] is not None and answer[\"property\"][\"name\"] == answer_text:\n",
    "                    # set predicted answer at rank = i+1 as correct answer\n",
    "                    prediction[\"correct_answer\"] = answer\n",
    "                    prediction[\"rank\"] = i+1\n",
    "                    break\n",
    "        \n",
    "        # finally, add prediction object to predictions list (again: predictions will contain for each tokenized sample of the batch one prediction object)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf697af-1823-4ced-8247-5c3020145742",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = [1,2,3,4,5,6,7,8,9,10]\n",
    "    \n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%Y_%d_%m-%H:%M:%S\")\n",
    "evaluation_log = os.path.join(evaluation_path, now+\".csv\")\n",
    "\n",
    "with open(evaluation_log, 'a') as f:\n",
    "    line = \"Checkpoint Path;\"\n",
    "    for k in top_k:\n",
    "        line += \"Rank@\"+str(k)+\";\"\n",
    "        line += \"Accuracy@\"+str(k)+\";\"\n",
    "        line += \"Accuracy Answerable Samples@\"+str(k)+\";\"\n",
    "        line += \"Accuracy Non Answerable Samples@\"+str(k)+\";\"\n",
    "        line += \"Correct Predictions@\"+str(k)+\";\"\n",
    "        line += \"Correct Predictions Answerable Samples@\"+str(k)+\";\"\n",
    "        line += \"Correct Predictions Non Answerable Samples@\"+str(k)+\";\"\n",
    "        line += \"Total Predictions@\"+str(k)+\";\"\n",
    "        line += \"Total Predictions Answerable Samples@\"+str(k)+\";\"\n",
    "        line += \"Total Predictions Non Answerable Samples@\"+str(k)+\";\"    \n",
    "    f.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7391b7-2310-4d83-891f-77ee258abd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in os.listdir(checkpoint_directory):\n",
    "    checkpoint_path = os.path.join(checkpoint_directory, directory)\n",
    "    if os.path.isdir(checkpoint_path):\n",
    "\n",
    "        results = []\n",
    "        for k in top_k:\n",
    "            results.append({\n",
    "                \"rank\": k,\n",
    "                \"accuracy\": 0,\n",
    "                \"accuracy_answerable_samples\": 0, \n",
    "                \"accuracy_non_answerable_samples\": 0,\n",
    "                \"correct_predictions\": 0,\n",
    "                \"correct_predictions_answerable_samples\": 0,\n",
    "                \"correct_predictions_non_answerable_samples\": 0,\n",
    "                \"total_predictions\": 0,\n",
    "                \"total_predictions_answerable_samples\": 0,\n",
    "                \"total_predictions_non_answerable_samples\": 0\n",
    "            })\n",
    "        \n",
    "        # load model from checkpoint\n",
    "        model = TFAutoModelForQuestionAnswering.from_pretrained(checkpoint_path)\n",
    "        print(\"Checkpoint '\",checkpoint_path,\"' loaded:\")\n",
    "        print(model.summary())\n",
    "                    \n",
    "\n",
    "        all_predictions = []\n",
    "        counter = 0\n",
    "        \n",
    "        batch = []\n",
    "\n",
    "        # Run prediction:\n",
    "        for validation_sample in tqdm(final_dataset):\n",
    "            if limit and counter > limit:\n",
    "                break\n",
    "\n",
    "            # append sample to batch\n",
    "            batch.append(validation_sample)\n",
    "            counter+=1\n",
    "            \n",
    "            # if sample size has reached maximum\n",
    "            if len(batch) == batch_size:\n",
    "                predictions = evaluate_sample_batch(batch, model)\n",
    "                all_predictions += predictions\n",
    "                \n",
    "                batch.clear()\n",
    "            #else:\n",
    "                # BUG (fixed in v4): sample must always be appended (i.e. before evaluating batch size), otherwise we loose a few tokenized samples\n",
    "                #batch.append(validation_sample)\n",
    "                #counter+=1\n",
    "                \n",
    "        # if there are unpredicted items remaining\n",
    "        if len(batch) != 0:\n",
    "            predictions = evaluate_sample_batch(batch, model)\n",
    "            all_predictions += predictions\n",
    "            batch.clear()\n",
    "\n",
    "        batch.clear()\n",
    "        \n",
    "        # Evaluate results:\n",
    "        for prediction in all_predictions:\n",
    "            for k in results:\n",
    "                if prediction[\"rank\"] is not None and prediction[\"rank\"] <= k[\"rank\"]:\n",
    "                    k[\"correct_predictions\"] += 1\n",
    "                    if prediction[\"theoretical_answer_out_of_span\"]:\n",
    "                        k[\"correct_predictions_non_answerable_samples\"] += 1\n",
    "                    else:\n",
    "                        k[\"correct_predictions_answerable_samples\"] += 1\n",
    "                        \n",
    "                k[\"total_predictions\"] += 1\n",
    "                if prediction[\"theoretical_answer_out_of_span\"]:\n",
    "                    k[\"total_predictions_non_answerable_samples\"] += 1\n",
    "                else:\n",
    "                    k[\"total_predictions_answerable_samples\"] +=1\n",
    "                        \n",
    "        # Finalize results\n",
    "        for k in results:\n",
    "            if k[\"total_predictions\"] > 0:\n",
    "                k[\"accuracy\"] = k[\"correct_predictions\"] / k[\"total_predictions\"]\n",
    "            if k[\"total_predictions_non_answerable_samples\"] > 0:\n",
    "                k[\"accuracy_non_answerable_samples\"] = k[\"correct_predictions_non_answerable_samples\"] / k[\"total_predictions_non_answerable_samples\"] \n",
    "            if k[\"total_predictions_answerable_samples\"] > 0:\n",
    "                k[\"accuracy_answerable_samples\"] = k[\"correct_predictions_answerable_samples\"] / k[\"total_predictions_answerable_samples\"] \n",
    "    \n",
    "\n",
    "        print(\"Total samples: \",len(all_predictions))\n",
    "        for k in results:\n",
    "            print(\"Accuracy@K \",k[\"rank\"],\": \",k[\"accuracy\"])\n",
    "        \n",
    "        with open(evaluation_log, 'a') as f:\n",
    "            line = checkpoint_path+\";\"\n",
    "            for k in results:\n",
    "                line += str(k[\"rank\"])+\";\"\n",
    "                line += str(k[\"accuracy\"])+\";\"\n",
    "                line += str(k[\"accuracy_answerable_samples\"])+\";\"\n",
    "                line += str(k[\"accuracy_non_answerable_samples\"])+\";\"\n",
    "                line += str(k[\"correct_predictions\"])+\";\"\n",
    "                line += str(k[\"correct_predictions_answerable_samples\"])+\";\"\n",
    "                line += str(k[\"correct_predictions_non_answerable_samples\"])+\";\"\n",
    "                line += str(k[\"total_predictions\"])+\";\"\n",
    "                line += str(k[\"total_predictions_answerable_samples\"])+\";\"\n",
    "                line += str(k[\"total_predictions_non_answerable_samples\"])+\";\"\n",
    "            f.write(line+\"\\n\")        \n",
    "        \n",
    "        global first_preprocessing_iteration\n",
    "        first_preprocessing_iteration = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7c4ca-d6c2-4a9c-a42a-9083961a5fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40ff668-78ef-4662-a3f2-2b5041e53d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hf]",
   "language": "python",
   "name": "conda-env-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
